{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"FLARES-2 Gridder","text":"<p>High-performance cosmological simulation gridding with spherical top hat kernels.</p>"},{"location":"#overview","title":"Overview","text":"<p>The FLARES-2 Gridder is a C++ application designed for gridding cosmological simulations. It applies spherical top hat kernels to matter distributions, reading HDF5 snapshot files from simulations (primarily SWIFT outputs) and computing overdensities at grid points using multiple kernel radii simultaneously.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multiple Kernel Radii: Compute overdensities for multiple smoothing scales in a single pass</li> <li>Hybrid Parallelization: OpenMP threading + optional MPI for distributed memory</li> <li>Efficient I/O: Chunked HDF5 reading with automatic optimization for sparse/dense grids</li> <li>Flexible Grids: Support for uniform, random, and file-based grid point distributions</li> <li>Octree Spatial Indexing: Hierarchical cell structure for efficient neighbor searches</li> <li>Production Ready: Comprehensive test suite with 29 tests covering serial and MPI modes</li> </ul>"},{"location":"#performance-characteristics","title":"Performance Characteristics","text":"Feature Single-Node (OpenMP) Multi-Node (MPI + OpenMP) Parallelization Multi-threaded Distributed + Multi-threaded Memory Shared Distributed with ghost cells I/O Chunked HDF5 Per-rank HDF5 files Scalability Up to ~16 cores Hundreds of cores"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Single-node with 8 OpenMP threads\nexport OMP_NUM_THREADS=8\n./build/parent_gridder params.yml 1\n\n# Multi-node: 4 MPI ranks \u00d7 2 OpenMP threads = 8 cores\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: Installation, quick start, and configuration</li> <li>Parameter Reference: Detailed parameter file documentation</li> <li>Performance: OpenMP and MPI optimization guides</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Parameter File Reference</li> <li>OpenMP Threading</li> <li>MPI Parallelization</li> </ul>"},{"location":"#support","title":"Support","text":"<p>For issues, questions, or contributions, please visit the GitHub repository.</p>"},{"location":"gridding/","title":"Gridding","text":"<p>Detailed guide to grid point generation modes and their use cases.</p>"},{"location":"gridding/#overview","title":"Overview","text":"<p>The gridder supports three methods for creating grid points where overdensities are calculated:</p> Type Description Use Case Memory Speed uniform Regular cubic lattice Dense sampling, maps High Fast random Monte Carlo sampling Statistics, sparse sampling Medium Fast file Custom coordinates Targeted regions, halos Variable Medium"},{"location":"gridding/#uniform-grids","title":"Uniform Grids","text":""},{"location":"gridding/#description","title":"Description","text":"<p>Creates a regular cubic lattice of grid points uniformly spanning the simulation volume.</p>"},{"location":"gridding/#configuration","title":"Configuration","text":"<pre><code>Grid:\n  type: uniform\n  cdim: 100  # Grid points per dimension\n</code></pre> <p>Creates <code>cdim \u00d7 cdim \u00d7 cdim</code> total grid points.</p>"},{"location":"gridding/#grid-structure","title":"Grid Structure","text":"<p>Points are placed at:</p> \\[ \\mathbf{r}_{i,j,k} = \\left( \\frac{i + 0.5}{N}, \\frac{j + 0.5}{N}, \\frac{k + 0.5}{N} \\right) \\times L_{\\rm box} \\] <p>where:</p> <ul> <li>\\(i, j, k \\in [0, N-1]\\) are grid indices</li> <li>\\(N\\) = <code>cdim</code></li> <li>\\(L_{\\rm box}\\) = simulation box size</li> <li>Points are cell-centered (offset by 0.5)</li> </ul>"},{"location":"gridding/#examples","title":"Examples","text":"Low Resolution (Quick Test)Medium Resolution (Standard)High Resolution (Production)Very High Resolution (HPC) <pre><code>Grid:\n  type: uniform\n  cdim: 50  # 125,000 points\n</code></pre> <ul> <li>Grid spacing: \\(L_{\\rm box} / 50\\)</li> <li>Memory: ~10 MB</li> <li>Use for: Quick tests, prototyping</li> </ul> <pre><code>Grid:\n  type: uniform\n  cdim: 100  # 1,000,000 points\n</code></pre> <ul> <li>Grid spacing: \\(L_{\\rm box} / 100\\)</li> <li>Memory: ~80 MB</li> <li>Use for: General analysis, intermediate maps</li> </ul> <pre><code>Grid:\n  type: uniform\n  cdim: 200  # 8,000,000 points\n</code></pre> <ul> <li>Grid spacing: \\(L_{\\rm box} / 200\\)</li> <li>Memory: ~640 MB</li> <li>Use for: Detailed maps, high-resolution analysis</li> </ul> <pre><code>Grid:\n  type: uniform\n  cdim: 500  # 125,000,000 points\n</code></pre> <ul> <li>Grid spacing: \\(L_{\\rm box} / 500\\)</li> <li>Memory: ~10 GB</li> <li>Use for: Ultra-high resolution, MPI required</li> </ul>"},{"location":"gridding/#resolution-selection","title":"Resolution Selection","text":"<p>Choose <code>cdim</code> based on science requirements:</p> <p>Nyquist Sampling:</p> <p>For kernel radius \\(R\\):</p> \\[ \\Delta x \\leq R \\quad \\Rightarrow \\quad {\\rm cdim} \\geq \\frac{L_{\\rm box}}{R} \\] <p>Example:</p> <ul> <li>Box size: 50 Mpc/h</li> <li>Kernel radius: 1.0 Mpc/h</li> <li>Minimum cdim: 50</li> </ul> <p>Recommended: Use 2-3\u00d7 Nyquist for well-sampled fields:</p> <pre><code># Box = 50 Mpc/h, kernel = 1.0 Mpc/h\n# Nyquist: cdim = 50\n# Recommended: cdim = 100-150\nGrid:\n  cdim: 100\n</code></pre>"},{"location":"gridding/#memory-scaling","title":"Memory Scaling","text":"<p>Memory requirements (approximate):</p> <pre><code>Points = cdim\u00b3\nMemory \u2248 Points \u00d7 (64 + 24 \u00d7 nkernels) bytes\n</code></pre> <p>Examples (5 kernels):</p> cdim Points Memory 50 125k 20 MB 100 1M 150 MB 200 8M 1.2 GB 300 27M 4 GB 500 125M 19 GB"},{"location":"gridding/#advantages","title":"Advantages","text":"<ul> <li>\u2705 Uniform coverage of simulation volume</li> <li>\u2705 Regular spacing ideal for FFTs, filtering</li> <li>\u2705 Predictable memory usage</li> <li>\u2705 Fast generation</li> <li>\u2705 Easy visualization (3D arrays)</li> </ul>"},{"location":"gridding/#disadvantages","title":"Disadvantages","text":"<ul> <li>\u274c Memory intensive for high resolution</li> <li>\u274c Samples low-density regions equally (inefficient)</li> <li>\u274c Fixed resolution everywhere</li> <li>\u274c Grid artifacts in power spectra</li> </ul>"},{"location":"gridding/#best-for","title":"Best For","text":"<ul> <li>Creating density field maps</li> <li>Uniform sampling requirements</li> <li>FFT-based post-processing</li> <li>Visual rendering</li> <li>Machine learning training data</li> </ul>"},{"location":"gridding/#random-grids","title":"Random Grids","text":""},{"location":"gridding/#description_1","title":"Description","text":"<p>Randomly distributed grid points using uniform sampling from the simulation volume.</p>"},{"location":"gridding/#configuration_1","title":"Configuration","text":"<pre><code>Grid:\n  type: random\n  n_grid_points: 1000000  # Total number of points\n</code></pre>"},{"location":"gridding/#distribution","title":"Distribution","text":"<p>Points are generated with:</p> \\[ \\mathbf{r}_i = (x_i, y_i, z_i), \\quad x_i, y_i, z_i \\sim \\mathcal{U}(0, L_{\\rm box}) \\] <p>where \\(\\mathcal{U}\\) is the uniform distribution.</p> <p>Random Number Generator: Mersenne Twister MT19937 (reproducible with same seed)</p>"},{"location":"gridding/#examples_1","title":"Examples","text":"Sparse SamplingMedium SamplingDense Sampling <pre><code>Grid:\n  type: random\n  n_grid_points: 100000  # 100k points\n</code></pre> <ul> <li>Average spacing: \\((V / N)^{1/3}\\)</li> <li>Use for: Quick statistics, correlation functions</li> </ul> <pre><code>Grid:\n  type: random\n  n_grid_points: 1000000  # 1M points\n</code></pre> <ul> <li>Good balance of coverage and cost</li> <li>Use for: PDF estimation, general statistics</li> </ul> <pre><code>Grid:\n  type: random\n  n_grid_points: 10000000  # 10M points\n</code></pre> <ul> <li>Near-uniform coverage</li> <li>Use for: High-fidelity statistical analysis</li> </ul>"},{"location":"gridding/#comparison-to-uniform","title":"Comparison to Uniform","text":"<p>For equivalent coverage:</p> <pre><code>Uniform: cdim\u00b3 points with spacing \u0394x = L_box / cdim\nRandom:  N points with average spacing (L_box\u00b3 / N)^(1/3)\n</code></pre> <p>Equivalent densities:</p> Uniform Random cdim=50 (125k) N=125k cdim=100 (1M) N=1M cdim=200 (8M) N=8M <p>Memory advantage: Random grids can achieve similar statistics with fewer points by avoiding over-sampling.</p>"},{"location":"gridding/#advantages_1","title":"Advantages","text":"<ul> <li>\u2705 No grid artifacts in power spectra</li> <li>\u2705 Flexible point count (not constrained to cubes)</li> <li>\u2705 Efficient for sparse sampling</li> <li>\u2705 Good for statistical analysis</li> <li>\u2705 Adaptable density (weight by local properties)</li> </ul>"},{"location":"gridding/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>\u274c Non-uniform coverage (clustering noise)</li> <li>\u274c Harder to visualize as maps</li> <li>\u274c No regular structure for FFTs</li> <li>\u274c Potentially poor sampling in low-density regions</li> </ul>"},{"location":"gridding/#best-for_1","title":"Best For","text":"<ul> <li>Statistical measurements (PDFs, correlation functions)</li> <li>Avoiding systematic errors from regular grids</li> <li>Memory-constrained situations</li> <li>Quick exploratory analysis</li> <li>Regions where uniform sampling is wasteful</li> </ul>"},{"location":"gridding/#seed-control","title":"Seed Control","text":"<p>The random seed is currently fixed (0) for reproducibility. To change:</p> <pre><code>// In construct_grid_points.cpp\nstd::mt19937 gen(0);  // Change this for different realizations\n</code></pre> <p>Future versions may expose seed as parameter.</p>"},{"location":"gridding/#file-based-grids","title":"File-Based Grids","text":""},{"location":"gridding/#description_2","title":"Description","text":"<p>Load grid point coordinates from a text file, enabling targeted sampling of specific regions.</p>"},{"location":"gridding/#configuration_2","title":"Configuration","text":"<pre><code>Grid:\n  type: file\n  grid_file: /path/to/grid_points.txt\n</code></pre>"},{"location":"gridding/#file-format","title":"File Format","text":"<p>Simple whitespace-delimited text file:</p> <pre><code># Comments start with #\n# Format: x y z (comoving Mpc/h, one point per line)\n10.5 20.3 15.7\n25.0 30.0 35.0\n12.1 18.9 22.3\n\n# Empty lines and whitespace are ignored\n\n30.5 35.2 40.1\n</code></pre> <p>Coordinate System:</p> <ul> <li>Same units as simulation (typically comoving Mpc/h)</li> <li>Origin at box corner (0, 0, 0)</li> <li>Must be within simulation boundaries</li> </ul> <p>Validation:</p> <p>Points outside the simulation box are automatically excluded with a warning:</p> <pre><code>Warning: Grid point (150.0, 25.0, 30.0) is outside simulation box (100.0\u00b3)\n</code></pre>"},{"location":"gridding/#placeholder-support","title":"Placeholder Support","text":"<p>Grid file paths support snapshot number replacement:</p> <pre><code>Input:\n  placeholder: \"0000\"\n\nGrid:\n  type: file\n  grid_file: /data/grids/grid_points_0000.txt\n</code></pre> <p>Command: <code>./parent_gridder params.yml 8 42</code></p> <p>Actual file: <code>/data/grids/grid_points_0042.txt</code></p>"},{"location":"gridding/#examples_2","title":"Examples","text":"Halo CentersSpecific SliceVoid CentersPencil Beam <p>Sample at locations of dark matter halos:</p> <pre><code># Python script to generate halo center grid\nimport h5py\nimport numpy as np\n\n# Read halo catalog\nwith h5py.File('halo_catalog_0042.hdf5', 'r') as f:\n    positions = f['Halos/Coordinates'][:]\n    masses = f['Halos/Mass'][:]\n\n# Select massive halos\nmask = masses &gt; 1e12  # &gt; 10^12 Msun\nhalo_positions = positions[mask]\n\n# Write grid file\nwith open('halo_centers_0042.txt', 'w') as f:\n    f.write(\"# Halo centers for snapshot 42\\n\")\n    f.write(\"# x y z (comoving Mpc/h)\\n\")\n    for pos in halo_positions:\n        f.write(f\"{pos[0]:.6f} {pos[1]:.6f} {pos[2]:.6f}\\n\")\n</code></pre> <p>Parameter file: <pre><code>Grid:\n  type: file\n  grid_file: halo_centers_0042.txt\n</code></pre></p> <p>Sample only a thin slice through the box:</p> <pre><code># Create grid points in a slice at z=50 Mpc/h\nimport numpy as np\n\n# Grid in x-y plane at fixed z\nnx, ny = 100, 100\nz_slice = 50.0\n\nwith open('slice_z50.txt', 'w') as f:\n    f.write(\"# Slice at z=50 Mpc/h\\n\")\n    for i in range(nx):\n        for j in range(ny):\n            x = (i + 0.5) * 100.0 / nx\n            y = (j + 0.5) * 100.0 / ny\n            f.write(f\"{x:.4f} {y:.4f} {z_slice:.4f}\\n\")\n</code></pre> <p>Sample in low-density regions:</p> <pre><code># Identify and sample void centers\n# (requires void-finding algorithm)\n\nvoid_centers = find_voids(snapshot)  # Your void finder\n\nwith open('void_centers.txt', 'w') as f:\n    f.write(\"# Void centers\\n\")\n    for center in void_centers:\n        f.write(f\"{center[0]} {center[1]} {center[2]}\\n\")\n</code></pre> <p>Sample along a line of sight:</p> <pre><code># Create points along a pencil beam\nn_points = 1000\nx0, y0 = 50.0, 50.0  # Fixed x, y\nz_min, z_max = 0.0, 100.0\n\nwith open('pencil_beam.txt', 'w') as f:\n    f.write(\"# Pencil beam through box center\\n\")\n    for i in range(n_points):\n        z = z_min + (z_max - z_min) * (i + 0.5) / n_points\n        f.write(f\"{x0} {y0} {z:.4f}\\n\")\n</code></pre>"},{"location":"gridding/#generation-tools","title":"Generation Tools","text":"<p>From Halo Catalogs:</p> <pre><code>import h5py\n\ndef extract_halo_positions(halo_file, output_file, min_mass=1e11):\n    \"\"\"Extract halo positions to grid file\"\"\"\n    with h5py.File(halo_file, 'r') as f:\n        pos = f['Halos/Coordinates'][:]\n        mass = f['Halos/Mass'][:]\n\n    mask = mass &gt; min_mass\n    selected_pos = pos[mask]\n\n    with open(output_file, 'w') as f:\n        f.write(f\"# Halo positions (M &gt; {min_mass:.1e} Msun)\\n\")\n        for p in selected_pos:\n            f.write(f\"{p[0]:.6f} {p[1]:.6f} {p[2]:.6f}\\n\")\n\n    print(f\"Wrote {len(selected_pos)} halo positions\")\n</code></pre> <p>From Density Field:</p> <pre><code>import numpy as np\n\ndef sample_overdense_regions(density_field, threshold, box_size):\n    \"\"\"Sample points in overdense regions\"\"\"\n    overdense = density_field &gt; threshold\n    indices = np.argwhere(overdense)\n\n    # Convert indices to coordinates\n    grid_points = (indices + 0.5) / density_field.shape[0] * box_size\n\n    np.savetxt('overdense_regions.txt', grid_points,\n               header='Overdense region centers',\n               fmt='%.6f')\n</code></pre>"},{"location":"gridding/#advantages_2","title":"Advantages","text":"<ul> <li>\u2705 Complete control over sampling</li> <li>\u2705 Target specific regions of interest</li> <li>\u2705 Adaptable point density</li> <li>\u2705 Reuse same points across snapshots</li> <li>\u2705 Can use external analysis (halo finders, etc.)</li> </ul>"},{"location":"gridding/#disadvantages_2","title":"Disadvantages","text":"<ul> <li>\u274c Requires external grid generation</li> <li>\u274c No automatic coverage</li> <li>\u274c File I/O overhead</li> <li>\u274c Must validate points are in box</li> </ul>"},{"location":"gridding/#best-for_2","title":"Best For","text":"<ul> <li>Targeted analysis (halos, voids, clusters)</li> <li>Following specific structures over time</li> <li>Custom sampling strategies</li> <li>Integrating with external pipelines</li> <li>Low-density sparse sampling</li> </ul>"},{"location":"gridding/#grid-comparison","title":"Grid Comparison","text":""},{"location":"gridding/#performance","title":"Performance","text":"Grid Type Generation Time Memory Search Time uniform (cdim=100) Fast (ms) High Fast random (1M) Fast (ms) Medium Fast file (1M) Medium (I/O) Medium Fast <p>Search time (finding particles in kernels) is similar for all types - dominated by octree traversal, not grid structure.</p>"},{"location":"gridding/#coverage","title":"Coverage","text":"Grid Type Uniformity Gaps Clustering uniform Perfect None None random Statistical Possible Poisson file Custom Variable Custom"},{"location":"gridding/#use-case-decision-tree","title":"Use Case Decision Tree","text":"<pre><code>graph TD\n    A[Choose Grid Type] --&gt; B{Need uniform coverage?}\n    B --&gt;|Yes| C{Memory available?}\n    B --&gt;|No| D{Target specific regions?}\n    C --&gt;|Yes| E[uniform&lt;br/&gt;Regular spacing]\n    C --&gt;|No| F[random&lt;br/&gt;Statistical sampling]\n    D --&gt;|Yes| G[file&lt;br/&gt;Custom points]\n    D --&gt;|No| F\n</code></pre>"},{"location":"gridding/#quick-recommendations","title":"Quick Recommendations","text":"<p>Use uniform when:</p> <ul> <li>Creating density maps/visualizations</li> <li>Need FFT-compatible output</li> <li>Analyzing entire volume uniformly</li> <li>Memory is not constrained</li> </ul> <p>Use random when:</p> <ul> <li>Measuring statistics (PDFs, correlations)</li> <li>Memory is limited</li> <li>Want to avoid grid artifacts</li> <li>Quick exploratory analysis</li> </ul> <p>Use file when:</p> <ul> <li>Following halos/structures</li> <li>Integrating with external analysis</li> <li>Need custom sampling</li> <li>Targeting specific regions</li> </ul>"},{"location":"gridding/#advanced-topics","title":"Advanced Topics","text":""},{"location":"gridding/#multi-resolution-grids","title":"Multi-Resolution Grids","text":"<p>Combine grid types by running multiple times:</p> <pre><code># High resolution in central region\n./parent_gridder params_center.yml 8\n\n# Low resolution in outskirts\n./parent_gridder params_outskirts.yml 8\n</code></pre>"},{"location":"gridding/#adaptive-sampling","title":"Adaptive Sampling","text":"<p>Use file-based grids with density-dependent spacing:</p> <pre><code># More points in high-density regions\ndef adaptive_grid(density_field, n_points, box_size):\n    # Probability proportional to density\n    prob = density_field / density_field.sum()\n    prob_flat = prob.flatten()\n\n    # Sample indices\n    indices_flat = np.random.choice(\n        len(prob_flat), size=n_points, p=prob_flat, replace=False\n    )\n\n    # Convert to 3D coordinates\n    indices = np.unravel_index(indices_flat, density_field.shape)\n    coords = (np.array(indices).T + 0.5) / density_field.shape[0] * box_size\n\n    return coords\n</code></pre>"},{"location":"gridding/#grid-refinement","title":"Grid Refinement","text":"<p>Iterative refinement around features:</p> <ol> <li>Start with coarse uniform grid</li> <li>Identify regions of interest</li> <li>Create file-based grid for refinement</li> <li>Combine results</li> </ol>"},{"location":"gridding/#validation","title":"Validation","text":"<p>Check grid coverage:</p> <pre><code>import h5py\nimport numpy as np\n\n# Load grid points from output\nwith h5py.File('gridded_output.hdf5', 'r') as f:\n    grid_coords = f['Grids/GridPointPositions'][:]\n\n# Plot distribution\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(grid_coords[:, 0], grid_coords[:, 1], grid_coords[:, 2],\n           s=1, alpha=0.5)\nax.set_xlabel('x [Mpc/h]')\nax.set_ylabel('y [Mpc/h]')\nax.set_zlabel('z [Mpc/h]')\nplt.title('Grid Point Distribution')\nplt.show()\n</code></pre>"},{"location":"gridding/#see-also","title":"See Also","text":"<ul> <li>Parameters - Grid parameter reference</li> <li>Quickstart - Basic examples</li> <li>Runtime Arguments - Command line options</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Complete installation guide for the FLARES-2 Gridder with various build configurations.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":""},{"location":"installation/#required-dependencies","title":"Required Dependencies","text":"Dependency Minimum Version Purpose CMake 3.12+ Build system C++ Compiler C++20 support Compilation (GCC 10+, Clang 11+, AppleClang 13+) HDF5 1.10+ Snapshot I/O OpenMP 4.5+ Multi-threading"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"Dependency Purpose When to Use MPI Multi-node parallelization Simulations &gt;10GB or &gt;16 cores needed Python 3 Test suite, utilities Development and testing h5py Test data generation Creating test snapshots"},{"location":"installation/#quick-install","title":"Quick Install","text":"Ubuntu/DebianmacOS (Homebrew)HPC Module System <pre><code># Install dependencies\nsudo apt-get update\nsudo apt-get install -y \\\n    cmake \\\n    g++ \\\n    libhdf5-dev \\\n    libomp-dev\n\n# Clone repository\ngit clone https://github.com/flaresimulations/gridder.git\ncd gridder\n\n# Build (single-node, OpenMP only)\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n</code></pre> <pre><code># Install dependencies\nbrew install cmake hdf5 libomp\n\n# Clone repository\ngit clone https://github.com/flaresimulations/gridder.git\ncd gridder\n\n# Build (single-node, OpenMP only)\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n</code></pre> <pre><code># Load modules (adjust names for your system)\nmodule load cmake/3.20\nmodule load gcc/11.2\nmodule load hdf5/1.12\nmodule load openmpi/4.1  # Optional, for MPI build\n\n# Clone repository\ngit clone https://github.com/flaresimulations/gridder.git\ncd gridder\n\n# Build (see build modes below)\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n</code></pre>"},{"location":"installation/#build-modes","title":"Build Modes","text":"<p>The gridder supports different build configurations depending on your needs.</p>"},{"location":"installation/#single-node-build-openmp-threading","title":"Single-Node Build (OpenMP Threading)","text":"<p>Best for: Desktop use, small simulations (&lt;10GB), up to ~16 cores</p> <pre><code>cmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n</code></pre> <p>Features: - Multi-threaded with OpenMP - Shared memory parallelization - Simple execution (no MPI required) - Single output HDF5 file</p> <p>Executable: <code>build/parent_gridder</code></p> <p>Usage: <pre><code># Auto-detect number of cores\n./build/parent_gridder params.yml 1\n\n# Specify 8 OpenMP threads explicitly\n./build/parent_gridder params.yml 8\n</code></pre></p>"},{"location":"installation/#multi-node-build-mpi-openmp-hybrid","title":"Multi-Node Build (MPI + OpenMP Hybrid)","text":"<p>Best for: HPC clusters, large simulations (&gt;10GB), hundreds of cores</p> <pre><code>cmake -B build_mpi -DENABLE_MPI=ON -DCMAKE_BUILD_TYPE=Release\ncmake --build build_mpi\n</code></pre> <p>Features: - Distributed memory parallelization (MPI) - Per-rank multi-threading (OpenMP) - Domain decomposition with ghost cells - Per-rank HDF5 output files + virtual file</p> <p>Executable: <code>build_mpi/parent_gridder</code></p> <p>Usage: <pre><code># 4 MPI ranks \u00d7 2 OpenMP threads = 8 cores total\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n\n# 16 ranks \u00d7 4 threads = 64 cores\nexport OMP_NUM_THREADS=4\nmpirun -n 16 ./build_mpi/parent_gridder params.yml 1\n</code></pre></p>"},{"location":"installation/#debug-build","title":"Debug Build","text":"<p>For development and debugging:</p> <pre><code>cmake -B build_debug -DCMAKE_BUILD_TYPE=Debug\ncmake --build build_debug\n</code></pre> <p>Features: - Debug symbols (<code>-g</code>) - No optimization (<code>-O0</code>) - Additional runtime checks (<code>DEBUGGING_CHECKS</code> enabled) - Verbose error messages</p> <p>Note: 10-100\u00d7 slower than Release build</p>"},{"location":"installation/#build-types-summary","title":"Build Types Summary","text":"Build Type Optimization Debug Symbols Use Case <code>Release</code> <code>-O3 -march=native</code> No Production (default) <code>Debug</code> <code>-O0</code> Yes Development, debugging <code>RelWithDebInfo</code> <code>-O2</code> Yes Performance profiling <code>MinSizeRel</code> <code>-Os</code> No Minimal binary size"},{"location":"installation/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"installation/#specifying-compiler","title":"Specifying Compiler","text":"<pre><code># Use specific compiler\nCC=gcc-11 CXX=g++-11 cmake -B build\ncmake --build build\n</code></pre>"},{"location":"installation/#custom-hdf5-location","title":"Custom HDF5 Location","text":"<p>If HDF5 is not found automatically:</p> <pre><code>cmake -B build \\\n  -DHDF5_ROOT=/path/to/hdf5 \\\n  -DCMAKE_BUILD_TYPE=Release\ncmake --build build\n</code></pre>"},{"location":"installation/#custom-build-flags","title":"Custom Build Flags","text":"<pre><code>cmake -B build \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCMAKE_CXX_FLAGS=\"-O3 -mavx2\" \\  # Custom optimization\n  -DCMAKE_INSTALL_PREFIX=/opt/gridder  # Install location\ncmake --build build\n</code></pre>"},{"location":"installation/#install-to-system","title":"Install to System","text":"<pre><code>cmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build\nsudo cmake --install build  # Installs to CMAKE_INSTALL_PREFIX\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#test-the-build","title":"Test the Build","text":"<p>Quick test with the simple test suite:</p> <pre><code># Single-node build\nbash tests/run_simple_test.sh\n\n# MPI build (requires MPI)\ncd tests &amp;&amp; bash run_mpi_tests.sh\n</code></pre>"},{"location":"installation/#check-version","title":"Check Version","text":"<pre><code>./build/parent_gridder --help\n</code></pre> <p>Should display: <pre><code>FLARES-2 Parent Gridder\nVersion: 0.1.0\nGit: &lt;commit&gt; (&lt;branch&gt;)\n\nUsage: parent_gridder &lt;parameter_file&gt; &lt;nthreads&gt; [snapshot_number] [verbosity]\n...\n</code></pre></p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"CMake can't find HDF5 <p>Symptoms: <pre><code>Could NOT find HDF5 (missing: HDF5_LIBRARIES HDF5_INCLUDE_DIRS)\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install HDF5 development headers:    <pre><code># Ubuntu/Debian\nsudo apt-get install libhdf5-dev\n\n# macOS\nbrew install hdf5\n</code></pre></p> </li> <li> <p>Specify HDF5 location explicitly:    <pre><code>cmake -B build -DHDF5_ROOT=/path/to/hdf5\n</code></pre></p> </li> <li> <p>On HPC, load HDF5 module:    <pre><code>module load hdf5\nmodule show hdf5  # Shows HDF5_ROOT path\n</code></pre></p> </li> </ol> OpenMP not found <p>Symptoms: <pre><code>Could NOT find OpenMP_CXX\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install OpenMP:    <pre><code># Ubuntu/Debian\nsudo apt-get install libomp-dev\n\n# macOS\nbrew install libomp\n</code></pre></p> </li> <li> <p>For macOS with AppleClang, ensure libomp is in path:    <pre><code>export OpenMP_ROOT=$(brew --prefix)/opt/libomp\ncmake -B build -DOpenMP_ROOT=$OpenMP_ROOT\n</code></pre></p> </li> </ol> C++20 not supported <p>Symptoms: <pre><code>error: This file requires compiler and library support for the ISO C++ 2020 standard.\n</code></pre></p> <p>Solution: Update your compiler: <pre><code># Ubuntu/Debian\nsudo apt-get install g++-11  # Or g++-12, g++-13\nCC=gcc-11 CXX=g++-11 cmake -B build\n</code></pre></p> MPI build fails <p>Symptoms: <pre><code>Could NOT find MPI_CXX\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install MPI:    <pre><code># Ubuntu/Debian\nsudo apt-get install libopenmpi-dev\n\n# macOS\nbrew install open-mpi\n</code></pre></p> </li> <li> <p>Load MPI module on HPC:    <pre><code>module load openmpi  # or mpich, intel-mpi, etc.\n</code></pre></p> </li> <li> <p>Specify MPI compiler explicitly:    <pre><code>cmake -B build_mpi \\\n  -DENABLE_MPI=ON \\\n  -DMPI_CXX_COMPILER=mpicxx\n</code></pre></p> </li> </ol> Link errors with HDF5 <p>Symptoms: <pre><code>undefined reference to `H5Fopen'\n</code></pre></p> <p>Solution: Ensure HDF5 C and HL (high-level) libraries are linked. On some systems: <pre><code>cmake -B build \\\n  -DHDF5_USE_STATIC_LIBRARIES=OFF \\\n  -DCMAKE_BUILD_TYPE=Release\n</code></pre></p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Get started with a simple example</li> <li>Parameters - Learn about parameter file configuration</li> <li>Runtime Arguments - Understand command line options</li> <li>MPI - Detailed MPI usage and optimization</li> </ul>"},{"location":"installation/#build-configuration-summary","title":"Build Configuration Summary","text":"<p>After successful build, CMake displays a configuration summary:</p> <pre><code>=== Configuration Summary ===\nProject: ZoomParentGridder\nVersion: 0.1.0\nBuild type: Release\nC++ standard: 20\nCompiler: GNU@11.2.0\nMPI enabled: ON/OFF\nHDF5 version: 1.12.1\nOpenMP enabled: TRUE\nGit revision: abc1234\nGit branch: main\n===============================\n</code></pre> <p>Verify this matches your requirements before proceeding.</p>"},{"location":"mpi/","title":"MPI Parallelization","text":"<p>Complete guide to using the gridder with MPI for multi-node execution.</p>"},{"location":"mpi/#overview","title":"Overview","text":"<p>The MPI build enables distributed memory parallelization across multiple compute nodes, allowing:</p> <ul> <li>Processing simulations larger than single-node memory</li> <li>Scaling to hundreds of cores</li> <li>Faster execution on HPC clusters</li> </ul> <p>Hybrid Approach: MPI (inter-node) + OpenMP (intra-node threading)</p>"},{"location":"mpi/#when-to-use-mpi","title":"When to Use MPI","text":"Use Case Recommendation Simulation &lt; 10 GB Single-node (OpenMP only) is simpler and often faster Simulation 10-100 GB MPI beneficial, especially if &gt;16 cores needed Simulation &gt; 100 GB MPI required (memory distribution essential) Grid points &gt; 10M MPI helps distribute computation Available cores &gt; 16 MPI + OpenMP hybrid scales better <p>Rule of thumb: Use MPI when you need more than one node's worth of resources.</p>"},{"location":"mpi/#build-for-mpi","title":"Build for MPI","text":""},{"location":"mpi/#compilation","title":"Compilation","text":"<pre><code>cmake -B build_mpi -DENABLE_MPI=ON -DCMAKE_BUILD_TYPE=Release\ncmake --build build_mpi\n</code></pre> <p>This creates <code>build_mpi/parent_gridder</code> with MPI support.</p> <p>Verification:</p> <pre><code>./build_mpi/parent_gridder --version | grep MPI\n# Should show: MPI: Enabled (OpenMPI X.Y.Z)\n</code></pre>"},{"location":"mpi/#execution","title":"Execution","text":""},{"location":"mpi/#basic-usage","title":"Basic Usage","text":"<pre><code># Set OpenMP threads per rank\nexport OMP_NUM_THREADS=2\n\n# Run with 4 MPI ranks\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1 42\n</code></pre> <p>Total cores used: <code>num_ranks \u00d7 OMP_NUM_THREADS</code></p> <p>Example: 4 ranks \u00d7 2 threads = 8 cores</p>"},{"location":"mpi/#slurm-batch-script","title":"SLURM Batch Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=gridder\n#SBATCH --nodes=2              # Number of nodes\n#SBATCH --ntasks-per-node=16   # MPI ranks per node\n#SBATCH --cpus-per-task=2      # OpenMP threads per rank\n#SBATCH --time=02:00:00\n#SBATCH --mem-per-cpu=4G\n\n# Load modules\nmodule load gcc/11.2\nmodule load hdf5/1.12\nmodule load openmpi/4.1\n\n# Set OpenMP configuration\nexport OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}\nexport OMP_PROC_BIND=close\nexport OMP_PLACES=cores\n\n# Total: 2 nodes \u00d7 16 ranks/node \u00d7 2 threads/rank = 64 cores\nsrun ./build_mpi/parent_gridder params.yml 1 ${SNAPSHOT}\n</code></pre>"},{"location":"mpi/#pbstorque-batch-script","title":"PBS/Torque Batch Script","text":"<pre><code>#!/bin/bash\n#PBS -N gridder\n#PBS -l nodes=4:ppn=16\n#PBS -l walltime=02:00:00\n\ncd $PBS_O_WORKDIR\n\n# Load modules\nmodule load gcc/11.2\nmodule load hdf5/1.12\nmodule load openmpi/4.1\n\n# 4 nodes \u00d7 16 cores/node = 64 total ranks\nexport OMP_NUM_THREADS=1\nmpirun -np 64 ./build_mpi/parent_gridder params.yml 1 42\n</code></pre>"},{"location":"mpi/#mpi-workflow","title":"MPI Workflow","text":""},{"location":"mpi/#domain-decomposition","title":"Domain Decomposition","text":"<p>The simulation volume is partitioned across MPI ranks using a space-filling curve:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Rank 0  \u2502 Rank 1  \u2502  Top-level cells assigned\n\u2502  cells  \u2502  cells  \u2502  to ranks using Hilbert\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  curve for spatial locality\n\u2502 Rank 2  \u2502 Rank 3  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits:</p> <ul> <li>Spatially local cells grouped together (better cache performance)</li> <li>Load balanced (approximately equal particles per rank)</li> <li>Minimizes inter-rank communication</li> </ul>"},{"location":"mpi/#execution-phases","title":"Execution Phases","text":"<pre><code>graph TD\n    A[Rank 0: Read metadata] --&gt; B[Broadcast to all ranks]\n    B --&gt; C[All ranks: Create grid points]\n    C --&gt; D[Assign cells to ranks]\n    D --&gt; E[Each rank: Load local particles]\n    E --&gt; F[Flag useful cells]\n    F --&gt; G[Exchange ghost cell data]\n    G --&gt; H[Build octree per rank]\n    H --&gt; I[Compute overdensities]\n    I --&gt; J[Each rank: Write output file]\n    J --&gt; K[Rank 0: Create virtual HDF5]\n</code></pre> <p>Key Points:</p> <ol> <li>Metadata read once - Only rank 0 reads, then broadcasts</li> <li>Particles read per-rank - Each rank loads only its cells' particles</li> <li>Ghost cells - Ranks exchange particles near boundaries</li> <li>Independent output - Each rank writes its own HDF5 file</li> <li>Virtual file - Rank 0 creates combined view</li> </ol>"},{"location":"mpi/#ghost-cells","title":"Ghost Cells","text":"<p>Cells near rank boundaries need particles from neighboring ranks:</p> <pre><code>Rank 0        \u2502  Rank 1\n              \u2502\n   Cell A  \u2500\u2500\u2500\u253c\u2500\u2500&gt; Needs particles\n   (needs     \u2502     from Rank 1\n    neighbor  \u2502     for large kernels)\n    data)     \u2502\n</code></pre> <p>Ghost Exchange Process:</p> <ol> <li>Identify cells within <code>max_kernel_radius</code> of boundaries</li> <li>Flag as \"proxy cells\" on other ranks</li> <li>Send/receive particle data for proxy cells</li> <li>Use ghost particles for accurate kernel calculations</li> </ol> <p>Communication Pattern: Point-to-point (each rank communicates only with neighbors)</p>"},{"location":"mpi/#output-files","title":"Output Files","text":""},{"location":"mpi/#per-rank-files","title":"Per-Rank Files","text":"<p>Each MPI rank writes a separate HDF5 file:</p> <pre><code>output_directory/\n\u251c\u2500\u2500 gridded_data_rank0.hdf5\n\u251c\u2500\u2500 gridded_data_rank1.hdf5\n\u251c\u2500\u2500 gridded_data_rank2.hdf5\n\u2514\u2500\u2500 gridded_data_rank3.hdf5\n</code></pre> <p>Contents: Only the grid points and cells owned by that rank.</p>"},{"location":"mpi/#virtual-file","title":"Virtual File","text":"<p>Rank 0 creates a master file that combines all rank files:</p> <pre><code>gridded_data.hdf5  \u2190 Virtual file (lightweight, no data duplication)\n</code></pre> <p>Reading:</p> <pre><code>import h5py\n\n# Read virtual file (appears as single dataset)\nwith h5py.File('gridded_data.hdf5', 'r') as f:\n    # Automatically reads from all rank files\n    overdens = f['Grids/Kernel_0/GridPointOverDensities'][:]\n    positions = f['Grids/GridPointPositions'][:]\n</code></pre> <p>Advantages:</p> <ul> <li>No data duplication (virtual file is small)</li> <li>Transparent to users (looks like single file)</li> <li>Parallel I/O still possible with per-rank files</li> </ul>"},{"location":"mpi/#performance-optimization","title":"Performance Optimization","text":""},{"location":"mpi/#rank-count-selection","title":"Rank Count Selection","text":"<p>Strong Scaling (Fixed Problem Size):</p> <pre><code># 1M grid points, varying ranks\n# 1 rank:   100s (baseline)\n# 2 ranks:  55s  (1.8\u00d7 speedup)\n# 4 ranks:  30s  (3.3\u00d7 speedup)\n# 8 ranks:  18s  (5.6\u00d7 speedup)\n# 16 ranks: 12s  (8.3\u00d7 speedup)\n# 32 ranks: 10s  (10\u00d7 speedup) \u2190 diminishing returns\n</code></pre> <p>Weak Scaling (Proportional Increase):</p> <pre><code># Scale grid points with ranks\n# 1 rank,  1M points:   100s\n# 2 ranks, 2M points:   105s  (similar time)\n# 4 ranks, 4M points:   110s  (good scaling)\n# 8 ranks, 8M points:   120s  (acceptable)\n</code></pre> <p>Recommendations:</p> <ul> <li>Minimum: ~100k grid points per rank (avoid over-decomposition)</li> <li>Maximum: Limited by communication overhead (~128-256 ranks)</li> <li>Sweet spot: 500k - 5M grid points per rank</li> </ul>"},{"location":"mpi/#thread-count-per-rank","title":"Thread Count per Rank","text":"<p>Hybrid Configuration:</p> <pre><code># Node has 32 cores total\n# Option 1: Many ranks, few threads\nexport OMP_NUM_THREADS=1\nmpirun -n 32 ./parent_gridder params.yml 1\n\n# Option 2: Balanced (usually best)\nexport OMP_NUM_THREADS=4\nmpirun -n 8 ./parent_gridder params.yml 1\n\n# Option 3: Few ranks, many threads\nexport OMP_NUM_THREADS=16\nmpirun -n 2 ./parent_gridder params.yml 1\n</code></pre> <p>General Rule:</p> <ul> <li>More ranks: Better for I/O-bound, irregular workloads</li> <li>More threads: Better for compute-bound, regular workloads</li> <li>Balanced: 4-8 threads per rank often optimal</li> </ul> <p>Test on Your System:</p> <pre><code>#!/bin/bash\n# Benchmark different configurations\nfor NRANKS in 1 2 4 8 16; do\n    NTHREADS=$((32 / NRANKS))\n    export OMP_NUM_THREADS=$NTHREADS\n    echo \"Testing $NRANKS ranks \u00d7 $NTHREADS threads\"\n    time mpirun -n $NRANKS ./parent_gridder params.yml 1\ndone\n</code></pre>"},{"location":"mpi/#io-optimization","title":"I/O Optimization","text":"<p>Chunked Reading:</p> <p>The gridder automatically optimizes particle I/O using chunked reads:</p> <pre><code># Control chunk merging (in metadata, not user-settable currently)\n# gap_fill_fraction: 0.1  # Merge chunks if gap &lt; 10% of total\n</code></pre> <p>Future: User control via parameter file.</p> <p>Performance Impact:</p> <ul> <li>Sparse grids: May read some unused particles (acceptable overhead)</li> <li>Dense grids: Reads all particles anyway (no overhead)</li> <li>Benefit: 10-100\u00d7 fewer HDF5 read operations</li> </ul>"},{"location":"mpi/#memory-management","title":"Memory Management","text":"<p>Per-Rank Memory:</p> <pre><code>Memory = Simulation/N_ranks + Overhead\n</code></pre> <p>Example:</p> <ul> <li>Simulation: 40 GB</li> <li>8 ranks: ~5 GB per rank + overhead</li> <li>Total cluster memory: 8 \u00d7 (5 + 2) = 56 GB</li> </ul> <p>Overhead includes:</p> <ul> <li>Octree structure (~20-30% of particle data)</li> <li>Grid points (~100 MB per million points)</li> <li>Ghost cells (~10-20% of particles)</li> <li>Communication buffers</li> </ul> <p>Rule of thumb: Request 1.5\u00d7 expected per-rank particle memory.</p>"},{"location":"mpi/#troubleshooting","title":"Troubleshooting","text":"MPI_Init failed <p>Error: <pre><code>ORTE_ERROR_LOG: Not found\n</code></pre></p> <p>Cause: MPI not properly initialized.</p> <p>Solution:</p> <ol> <li> <p>Ensure MPI module is loaded:    <pre><code>module load openmpi  # Or your MPI flavor\n</code></pre></p> </li> <li> <p>Use <code>mpirun</code> or <code>srun</code>, not direct execution:    <pre><code># Wrong:\n./build_mpi/parent_gridder params.yml 8\n\n# Right:\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n</code></pre></p> </li> </ol> Ranks hanging at MPI_Barrier <p>Symptom: Program freezes, no output.</p> <p>Cause: Deadlock from mismatched MPI calls or errors on some ranks.</p> <p>Debug:</p> <pre><code># Run with high verbosity\nexport OMP_NUM_THREADS=1\nmpirun -n 2 ./parent_gridder params.yml 1 0 2  # verbosity=2\n</code></pre> <p>Common causes:</p> <ul> <li>One rank encounters error, others wait forever</li> <li>File I/O fails on some nodes</li> <li>Different parameter files on different ranks</li> </ul> <p>Solution: Ensure all ranks can access all files.</p> Output files not created <p>Symptom: Some rank files missing.</p> <p>Cause: Rank failed but didn't propagate error.</p> <p>Debug:</p> <pre><code># Check stderr from all ranks\nmpirun -n 4 ./parent_gridder params.yml 1 2&gt;&amp;1 | tee output.log\ngrep -i error output.log\n</code></pre> Poor scaling / No speedup <p>Symptom: 4 ranks barely faster than 1 rank.</p> <p>Causes:</p> <ol> <li>Too few grid points per rank: <pre><code># Bad: 10k points \u00f7 16 ranks = 625 points/rank\nGrid:\n  cdim: 22  # Only 10,648 points total\n</code></pre></li> </ol> <p>Fix: Use fewer ranks or more grid points.</p> <ol> <li>Large kernels causing ghost overhead:</li> </ol> <p>Check ghost cell fraction in output:    <pre><code>[INFO] Ghost cells: 45% of total\n</code></pre></p> <p>If &gt;30%, consider:    - Smaller kernels    - Fewer ranks    - Larger cells (higher <code>max_leaf_count</code>)</p> <ol> <li>I/O bottleneck:</li> </ol> <p>All ranks reading same file simultaneously. Use:    - Lustre striping on HPC    - Parallel filesystem (not NFS)</p> How many ranks should I use? <p>Quick formula:</p> <pre><code>N_ranks \u2248 GridPoints / 500,000\nN_ranks \u2248 SimulationGB / 5\n</code></pre> <p>Use the smaller of the two.</p> <p>Example:</p> <ul> <li>2M grid points, 20 GB simulation</li> <li>By points: 2M / 500k = 4 ranks</li> <li>By memory: 20 / 5 = 4 ranks</li> <li>Use: 4 ranks</li> </ul>"},{"location":"mpi/#advanced-mpi","title":"Advanced MPI","text":""},{"location":"mpi/#custom-mpi-launcher","title":"Custom MPI Launcher","text":"<p>Some systems use different launchers:</p> <pre><code># OpenMPI\nmpirun -n 4 ./parent_gridder params.yml 1\n\n# Intel MPI\nmpiexec -n 4 ./parent_gridder params.yml 1\n\n# SLURM\nsrun -n 4 ./parent_gridder params.yml 1\n\n# PBS/Torque\nmpirun -np 4 ./parent_gridder params.yml 1\n</code></pre>"},{"location":"mpi/#process-binding","title":"Process Binding","text":"<p>Bind ranks to cores:</p> <pre><code># OpenMPI\nmpirun -n 4 --bind-to core ./parent_gridder params.yml 1\n\n# With rank-to-core mapping\nmpirun -n 4 --map-by core --bind-to core ./parent_gridder params.yml 1\n</code></pre> <p>Benefits:</p> <ul> <li>Prevents thread migration (better cache performance)</li> <li>Avoids hyperthreading (use physical cores)</li> </ul>"},{"location":"mpi/#network-optimization","title":"Network Optimization","text":"<p>InfiniBand:</p> <pre><code># Force use of InfiniBand\nexport OMPI_MCA_btl=self,openib\nmpirun -n 16 ./parent_gridder params.yml 1\n</code></pre> <p>Ethernet:</p> <pre><code># Use TCP for communication\nexport OMPI_MCA_btl=self,tcp\nmpirun -n 8 ./parent_gridder params.yml 1\n</code></pre>"},{"location":"mpi/#debugging-mpi-issues","title":"Debugging MPI Issues","text":"<p>Verbose MPI output:</p> <pre><code># OpenMPI debug info\nexport OMPI_MCA_btl_base_verbose=30\nmpirun -n 2 ./parent_gridder params.yml 1\n</code></pre> <p>Attach debugger:</p> <pre><code># GDB on rank 0\nmpirun -n 4 xterm -e gdb ./parent_gridder\n</code></pre> <p>Valgrind with MPI:</p> <pre><code>mpirun -n 2 valgrind --leak-check=full ./parent_gridder params.yml 1\n</code></pre>"},{"location":"mpi/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"mpi/#example-50-mpch-box-100m-particles","title":"Example: 50 Mpc/h Box, 100M Particles","text":"Configuration Grid Points Time Memory/Rank 1 rank \u00d7 16 threads 1M 180s 8 GB 4 ranks \u00d7 4 threads 1M 52s 2.5 GB 8 ranks \u00d7 2 threads 1M 30s 1.5 GB 16 ranks \u00d7 1 thread 1M 22s 1 GB <p>Speedup: ~8\u00d7 from 1 to 16 ranks (good strong scaling)</p>"},{"location":"mpi/#example-weak-scaling-test","title":"Example: Weak Scaling Test","text":"Ranks Grid Points Particles Time Efficiency 1 1M 100M 180s 100% 2 2M 200M 190s 95% 4 4M 400M 200s 90% 8 8M 800M 220s 82% <p>Result: Good weak scaling up to 8 ranks, then communication overhead grows.</p>"},{"location":"mpi/#see-also","title":"See Also","text":"<ul> <li>Installation - Building with MPI</li> <li>Runtime Arguments - Command line options</li> <li>Parameters - Configuration reference</li> <li>Quickstart - Basic examples</li> </ul>"},{"location":"parameters/","title":"Parameter Reference","text":"<p>Complete reference for all parameter file options.</p>"},{"location":"parameters/#overview","title":"Overview","text":"<p>Parameter files use YAML syntax and define:</p> <ul> <li>Kernels: Smoothing radii for overdensity calculations</li> <li>Grid: Grid point generation method and resolution</li> <li>Tree: Octree spatial indexing configuration</li> <li>Input: Simulation snapshot file path</li> <li>Output: Output file location and options</li> </ul>"},{"location":"parameters/#complete-example","title":"Complete Example","text":"<pre><code>Kernels:\n  nkernels: 3\n  kernel_radius_1: 0.5\n  kernel_radius_2: 1.0\n  kernel_radius_3: 2.0\n\nGrid:\n  type: uniform\n  cdim: 100\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: /path/to/snapshot_0042.hdf5\n  placeholder: \"0042\"\n\nOutput:\n  filepath: ./output/\n  basename: gridded_data.hdf5\n  write_masses: 1\n</code></pre>"},{"location":"parameters/#kernels","title":"Kernels","text":"<p>Defines the spherical top hat kernel radii for overdensity calculations.</p>"},{"location":"parameters/#nkernels","title":"<code>nkernels</code>","text":"<p>Required Parameter</p> <p>Type: Integer Range: 1 to 100 (practical limit) Description: Number of kernel radii to use</p> <p>Each kernel radius produces a separate overdensity field. Multiple radii enable multi-scale analysis in a single run.</p> <p>Example: <pre><code>Kernels:\n  nkernels: 5  # Will use 5 different kernel radii\n</code></pre></p> <p>Performance Note: Each kernel adds ~linear overhead. 10 kernels \u2248 10\u00d7 slower than 1 kernel.</p>"},{"location":"parameters/#kernel_radius_n","title":"<code>kernel_radius_N</code>","text":"<p>Required Parameters (N=1 to nkernels)</p> <p>Type: Float Units: Comoving Mpc/h (simulation units) Range: &gt; 0.0 Description: Radius of the N-th spherical top hat kernel</p> <p>Kernel radii must be numbered consecutively starting from 1. The gridder counts particles within a sphere of this radius around each grid point.</p> <p>Example: <pre><code>Kernels:\n  nkernels: 3\n  kernel_radius_1: 0.5   # Small scale (0.5 Mpc/h)\n  kernel_radius_2: 1.0   # Medium scale (1.0 Mpc/h)\n  kernel_radius_3: 2.0   # Large scale (2.0 Mpc/h)\n</code></pre></p> <p>Physics Notes:</p> <ul> <li>Typical values: 0.1 - 10.0 Mpc/h</li> <li>Smaller radii: Higher resolution, noisier</li> <li>Larger radii: Smoother fields, lower resolution</li> <li>Choose based on science goals (e.g., halo masses, large-scale structure)</li> </ul> <p>Performance Impact:</p> <ul> <li>Larger radii = more particles per grid point = slower</li> <li>Octree optimization helps, but 10 Mpc/h kernels will be much slower than 0.5 Mpc/h</li> </ul>"},{"location":"parameters/#grid","title":"Grid","text":"<p>Defines how grid points are generated or loaded.</p>"},{"location":"parameters/#type","title":"<code>type</code>","text":"<p>Required Parameter</p> <p>Type: String Options: <code>uniform</code>, <code>random</code>, <code>file</code> Description: Method for generating grid points</p> <p>See Gridding for detailed descriptions of each type.</p> <p>Example: <pre><code>Grid:\n  type: uniform  # Regular cubic grid\n</code></pre></p>"},{"location":"parameters/#grid-cdim","title":"<code>cdim</code>","text":"<p>Required when <code>type: uniform</code></p> <p>Type: Integer Range: 1 to ~1000 (limited by memory) Description: Grid points per dimension (creates <code>cdim\u00b3</code> total points)</p> <p>Example: <pre><code>Grid:\n  type: uniform\n  cdim: 100  # Creates 100\u00b3 = 1,000,000 grid points\n</code></pre></p> <p>Memory estimate: ~80 bytes per grid point (depends on nkernels)</p> <pre><code>cdim=50:   125,000 points \u2248 10 MB\ncdim=100:  1,000,000 points \u2248 80 MB\ncdim=200:  8,000,000 points \u2248 640 MB\ncdim=500:  125,000,000 points \u2248 10 GB\n</code></pre>"},{"location":"parameters/#n_grid_points","title":"<code>n_grid_points</code>","text":"<p>Required when <code>type: random</code></p> <p>Type: Integer Range: 1 to ~100,000,000 (limited by memory) Description: Total number of random grid points to generate</p> <p>Example: <pre><code>Grid:\n  type: random\n  n_grid_points: 1000000  # 1 million random points\n</code></pre></p> <p>Random points are uniformly distributed within the simulation box using a Mersenne Twister RNG.</p>"},{"location":"parameters/#grid_file","title":"<code>grid_file</code>","text":"<p>Required when <code>type: file</code></p> <p>Type: String (file path) Description: Path to text file containing grid point coordinates</p> <p>Example: <pre><code>Grid:\n  type: file\n  grid_file: /path/to/grid_points_0042.txt\n</code></pre></p> <p>File Format:</p> <pre><code># Comments start with #\n# Format: x y z (one point per line)\n10.5 20.3 15.7\n25.0 30.0 35.0\n# Whitespace and empty lines are ignored\n12.1 18.9 22.3\n</code></pre> <p>Coordinate Units: Same as simulation (typically comoving Mpc/h)</p> <p>Placeholder Support: Grid file paths support snapshot placeholders (see Input/placeholder)</p> <p>Example with placeholder: <pre><code>Input:\n  placeholder: \"0000\"\n\nGrid:\n  type: file\n  grid_file: /data/grids/grid_points_0000.txt\n  # When snapshot 42 is specified: grid_points_0042.txt\n</code></pre></p>"},{"location":"parameters/#tree","title":"Tree","text":"<p>Configures the octree spatial indexing structure.</p>"},{"location":"parameters/#max_leaf_count","title":"<code>max_leaf_count</code>","text":"<p>Required Parameter</p> <p>Type: Integer Range: 1 to 10000 (practical: 50-500) Description: Maximum particles allowed in a leaf cell before subdivision</p> <p>The octree recursively splits cells containing more than <code>max_leaf_count</code> particles until all cells meet this threshold or maximum depth is reached.</p> <p>Example: <pre><code>Tree:\n  max_leaf_count: 200\n</code></pre></p> <p>Performance Trade-offs:</p> Value Tree Depth Search Speed Memory Usage Build Time 50 Deeper Faster Higher Slower 200 Medium Medium Medium Medium 500 Shallower Slower Lower Faster <p>Recommendations:</p> <ul> <li>Default: 200 (good balance)</li> <li>Memory constrained: 300-500</li> <li>Speed critical: 100-150</li> <li>Very large simulations: 200-300</li> </ul> <p>Technical Details:</p> <p>Lower values create deeper trees with more cells but faster particle searches. Higher values create shallower trees with fewer cells but slower searches within each cell.</p> <p>Maximum tree depth is logged during execution: <pre><code>[INFO] Maximum depth in the tree: 12\n</code></pre></p>"},{"location":"parameters/#input","title":"Input","text":"<p>Defines the simulation snapshot file to process.</p>"},{"location":"parameters/#filepath","title":"<code>filepath</code>","text":"<p>Required Parameter</p> <p>Type: String (file path) Description: Path to HDF5 simulation snapshot</p> <p>Example: <pre><code>Input:\n  filepath: /scratch/simulations/FLARES/snapshot_0042.hdf5\n</code></pre></p> <p>Supported Formats:</p> <ul> <li>SWIFT HDF5 snapshots (primary target)</li> <li>Must contain <code>PartType1</code> group with <code>Coordinates</code> and <code>Masses</code> datasets</li> <li>Must contain <code>Header</code> with <code>BoxSize</code>, <code>Redshift</code> attributes</li> <li>Must contain <code>Units</code> and <code>Cosmology</code> groups</li> </ul> <p>Placeholder Replacement:</p> <p>If snapshot number is provided as command line argument, placeholders in the path are replaced:</p> <pre><code>Input:\n  filepath: /data/snapshots/snap_0000.hdf5\n  placeholder: \"0000\"\n</code></pre> <p>Command: <code>./parent_gridder params.yml 8 42</code> Actual path: <code>/data/snapshots/snap_0042.hdf5</code></p>"},{"location":"parameters/#input-placeholder","title":"<code>placeholder</code>","text":"<p>Optional Parameter</p> <p>Type: String Default: <code>\"0000\"</code> Description: String pattern in file paths to replace with snapshot number</p> <p>Example: <pre><code>Input:\n  filepath: /data/snapshot_XXXX.hdf5\n  placeholder: \"XXXX\"\n</code></pre></p> <p>Command: <code>./parent_gridder params.yml 8 42</code> Result: <code>/data/snapshot_0042.hdf5</code></p> <p>Padding:</p> <p>The snapshot number is zero-padded to match the placeholder length:</p> <pre><code>placeholder: \"000\"   # 3 digits: snap 5 \u2192 snap_005.hdf5\nplaceholder: \"0000\"  # 4 digits: snap 5 \u2192 snap_0005.hdf5\n</code></pre> <p>Multiple Occurrences:</p> <p>All instances of the placeholder in the path are replaced:</p> <pre><code>filepath: /data/0000/snapshot_0000.hdf5\nplaceholder: \"0000\"\n# snap 42 \u2192 /data/0042/snapshot_0042.hdf5\n</code></pre> <p>Applies To:</p> <ul> <li><code>Input/filepath</code></li> <li><code>Output/basename</code></li> <li><code>Grid/grid_file</code> (when <code>type: file</code>)</li> </ul>"},{"location":"parameters/#output","title":"Output","text":"<p>Configures output file location and content.</p>"},{"location":"parameters/#filepath_1","title":"<code>filepath</code>","text":"<p>Required Parameter</p> <p>Type: String (directory path) Description: Directory where output files will be written</p> <p>Example: <pre><code>Output:\n  filepath: ./output/\n</code></pre></p> <p>Notes:</p> <ul> <li>Directory must exist (not created automatically)</li> <li>Must have write permissions</li> <li>For MPI builds, per-rank files are written here (e.g., <code>output_rank0.hdf5</code>)</li> </ul>"},{"location":"parameters/#basename","title":"<code>basename</code>","text":"<p>Required Parameter</p> <p>Type: String (filename) Description: Base name for output HDF5 file(s)</p> <p>Example: <pre><code>Output:\n  filepath: ./results/\n  basename: gridded_snapshot.hdf5\n</code></pre></p> <p>Output: <code>./results/gridded_snapshot.hdf5</code></p> <p>MPI Mode:</p> <p>Per-rank files + virtual file: <pre><code>./results/gridded_snapshot_rank0.hdf5\n./results/gridded_snapshot_rank1.hdf5\n./results/gridded_snapshot_rank2.hdf5\n./results/gridded_snapshot.hdf5  # Virtual file combining all ranks\n</code></pre></p> <p>Placeholder Support:</p> <pre><code>Output:\n  basename: gridded_snap_0000.hdf5\n\nInput:\n  placeholder: \"0000\"\n</code></pre> <p>Command: <code>./parent_gridder params.yml 8 42</code> Output: <code>gridded_snap_0042.hdf5</code></p>"},{"location":"parameters/#write_masses","title":"<code>write_masses</code>","text":"<p>Optional Parameter</p> <p>Type: Integer (0 or 1) Default: <code>0</code> Description: Whether to write total mass within kernels</p> <p>Values:</p> <ul> <li><code>0</code>: Write only overdensities (default, saves space)</li> <li><code>1</code>: Write both overdensities and masses</li> </ul> <p>Example: <pre><code>Output:\n  write_masses: 1  # Include mass datasets\n</code></pre></p> <p>Output Datasets:</p> <p>When <code>write_masses: 0</code>: <pre><code>/Grids/Kernel_0/GridPointOverDensities\n/Grids/Kernel_1/GridPointOverDensities\n...\n</code></pre></p> <p>When <code>write_masses: 1</code>: <pre><code>/Grids/Kernel_0/GridPointOverDensities\n/Grids/Kernel_0/GridPointMasses\n/Grids/Kernel_1/GridPointOverDensities\n/Grids/Kernel_1/GridPointMasses\n...\n</code></pre></p> <p>Mass Units: \\(10^{10}\\) M\\(_\\odot\\)</p> <p>File Size Impact: Approximately doubles file size when enabled.</p>"},{"location":"parameters/#complete-parameter-examples","title":"Complete Parameter Examples","text":""},{"location":"parameters/#example-1-quick-uniform-grid","title":"Example 1: Quick Uniform Grid","text":"<p>Minimal setup for testing:</p> <pre><code>Kernels:\n  nkernels: 1\n  kernel_radius_1: 1.0\n\nGrid:\n  type: uniform\n  cdim: 50  # 125k points\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: snapshot.hdf5\n\nOutput:\n  filepath: ./\n  basename: test_grid.hdf5\n  write_masses: 0\n</code></pre>"},{"location":"parameters/#example-2-production-multi-scale-analysis","title":"Example 2: Production Multi-Scale Analysis","text":"<p>High-resolution with multiple scales:</p> <pre><code>Kernels:\n  nkernels: 5\n  kernel_radius_1: 0.25\n  kernel_radius_2: 0.5\n  kernel_radius_3: 1.0\n  kernel_radius_4: 2.0\n  kernel_radius_5: 4.0\n\nGrid:\n  type: uniform\n  cdim: 200  # 8 million points\n\nTree:\n  max_leaf_count: 150  # Deeper tree for speed\n\nInput:\n  filepath: /data/FLARES/snapshots/snap_0000.hdf5\n  placeholder: \"0000\"\n\nOutput:\n  filepath: /scratch/output/\n  basename: FLARES_gridded_0000.hdf5\n  write_masses: 1\n</code></pre>"},{"location":"parameters/#example-3-random-sampling-for-statistics","title":"Example 3: Random Sampling for Statistics","text":"<p>Monte Carlo approach:</p> <pre><code>Kernels:\n  nkernels: 3\n  kernel_radius_1: 0.5\n  kernel_radius_2: 1.0\n  kernel_radius_3: 2.0\n\nGrid:\n  type: random\n  n_grid_points: 5000000  # 5 million random points\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: snapshot_0050.hdf5\n\nOutput:\n  filepath: ./random_grids/\n  basename: random_grid_0050.hdf5\n  write_masses: 0\n</code></pre>"},{"location":"parameters/#example-4-custom-grid-points","title":"Example 4: Custom Grid Points","text":"<p>Target specific regions:</p> <pre><code>Kernels:\n  nkernels: 2\n  kernel_radius_1: 0.5\n  kernel_radius_2: 1.0\n\nGrid:\n  type: file\n  grid_file: /path/to/halo_centers_0000.txt\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: /data/snapshot_0000.hdf5\n  placeholder: \"0000\"\n\nOutput:\n  filepath: /results/halo_grids/\n  basename: halo_grid_0000.hdf5\n  write_masses: 1\n</code></pre>"},{"location":"parameters/#validation","title":"Validation","text":"<p>The gridder validates all parameters at startup and reports errors:</p> <p>Missing required parameter: <pre><code>ERROR: A required parameter was not set in the parameter file (Kernels/nkernels)\n</code></pre></p> <p>Invalid value: <pre><code>ERROR: Invalid grid type specified: uniformm\n(Did you mean 'uniform'?)\n</code></pre></p> <p>File not found: <pre><code>ERROR: Failed to open grid points file: /path/to/missing.txt\n</code></pre></p> <p>See Troubleshooting for common issues.</p>"},{"location":"parameters/#see-also","title":"See Also","text":"<ul> <li>Gridding - Detailed grid type descriptions</li> <li>Runtime Arguments - Command line options</li> <li>Quickstart - Get started quickly</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get up and running with the FLARES-2 Gridder in minutes.</p>"},{"location":"quickstart/#what-is-the-gridder","title":"What is the Gridder?","text":"<p>The FLARES-2 Gridder is a high-performance C++ application that computes overdensities from cosmological simulations at specified grid points using spherical top hat kernels. It reads HDF5 snapshot files (primarily from SWIFT simulations) and outputs gridded overdensity fields.</p>"},{"location":"quickstart/#basic-workflow","title":"Basic Workflow","text":"<pre><code>graph LR\n    A[Simulation Snapshot&lt;br/&gt;HDF5] --&gt; B[Gridder]\n    C[Parameter File&lt;br/&gt;YAML] --&gt; B\n    D[Grid Points&lt;br/&gt;Optional] --&gt; B\n    B --&gt; E[Gridded Output&lt;br/&gt;HDF5]\n</code></pre>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Built the gridder (see Installation)</li> <li>Access to a SWIFT simulation snapshot (HDF5 format)</li> <li>Basic understanding of YAML syntax</li> </ul>"},{"location":"quickstart/#minimal-example","title":"Minimal Example","text":""},{"location":"quickstart/#1-create-a-parameter-file","title":"1. Create a Parameter File","text":"<p>Create <code>params.yml</code> with minimal required parameters:</p> <pre><code>Kernels:\n  nkernels: 2\n  kernel_radius_1: 0.5  # Mpc/h\n  kernel_radius_2: 1.0  # Mpc/h\n\nGrid:\n  type: uniform\n  cdim: 50  # 50^3 = 125,000 grid points\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: /path/to/snapshot_0042.hdf5\n\nOutput:\n  filepath: ./output/\n  basename: gridded_snapshot.hdf5\n  write_masses: 0\n</code></pre>"},{"location":"quickstart/#2-run-the-gridder","title":"2. Run the Gridder","text":"Single-Node (OpenMP)Multi-Node (MPI) <pre><code># Use 8 OpenMP threads\n./build/parent_gridder params.yml 8\n</code></pre> <pre><code># 4 MPI ranks \u00d7 2 OpenMP threads = 8 cores total\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"quickstart/#3-check-the-output","title":"3. Check the Output","text":"<p>The gridder creates <code>./output/gridded_snapshot.hdf5</code> with:</p> <pre><code>/Grids/\n  \u251c\u2500\u2500 Kernel_0/\n  \u2502   \u2514\u2500\u2500 GridPointOverDensities  # Overdensity for 0.5 Mpc/h kernel\n  \u251c\u2500\u2500 Kernel_1/\n  \u2502   \u2514\u2500\u2500 GridPointOverDensities  # Overdensity for 1.0 Mpc/h kernel\n  \u2514\u2500\u2500 GridPointPositions          # (x, y, z) coordinates\n/Cells/\n  \u251c\u2500\u2500 GridPointCounts             # Number of grid points per cell\n  \u2514\u2500\u2500 GridPointStart              # Starting index for each cell\n/Header/                          # Metadata attributes\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Installation - Detailed installation instructions including MPI</li> <li>Parameters - Complete parameter reference</li> <li>Runtime Arguments - Command line options</li> <li>Gridding - Different grid types (uniform, random, file)</li> <li>MPI - Parallel execution on multiple nodes</li> </ul>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"quickstart/#high-resolution-uniform-grid","title":"High-Resolution Uniform Grid","text":"<p>For detailed density maps:</p> <pre><code>Grid:\n  type: uniform\n  cdim: 200  # 8 million grid points\n</code></pre>"},{"location":"quickstart/#multi-scale-analysis","title":"Multi-Scale Analysis","text":"<p>Multiple kernel radii for scale-dependent analysis:</p> <pre><code>Kernels:\n  nkernels: 5\n  kernel_radius_1: 0.25\n  kernel_radius_2: 0.5\n  kernel_radius_3: 1.0\n  kernel_radius_4: 2.0\n  kernel_radius_5: 4.0\n</code></pre>"},{"location":"quickstart/#random-sampling","title":"Random Sampling","text":"<p>For statistical analysis without regular grid artifacts:</p> <pre><code>Grid:\n  type: random\n  n_grid_points: 1000000  # 1 million random points\n</code></pre>"},{"location":"quickstart/#custom-grid-from-file","title":"Custom Grid from File","text":"<p>For targeted regions or non-uniform sampling:</p> <pre><code>Grid:\n  type: file\n  grid_file: /path/to/custom_grid_points.txt\n</code></pre> <p>Format of <code>custom_grid_points.txt</code>: <pre><code># x y z (one point per line, coordinates in simulation units)\n5.0 5.0 5.0\n10.2 15.3 20.1\n# Comments start with #\n25.0 30.0 35.0\n</code></pre></p>"},{"location":"quickstart/#performance-tips","title":"Performance Tips","text":"<p>Quick Performance Recommendations</p> <ul> <li>OpenMP threads: Set to number of physical cores (not hyperthreads)</li> <li>MPI ranks: Use for simulations &gt;10 GB or when needing &gt;16 cores</li> <li>max_leaf_count: 100-300 works well for most cases (lower = faster search, higher = less memory)</li> <li>Chunked I/O: Automatic - just set <code>gap_fill_fraction</code> (0.05-0.2) for sparse grids</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"Gridder crashes with 'out of memory' <ul> <li>Reduce grid resolution (<code>cdim</code>) or number of grid points</li> <li>Increase <code>max_leaf_count</code> (reduces tree depth, saves memory)</li> <li>Use MPI to distribute memory across nodes</li> </ul> Slow runtime <ul> <li>Increase OpenMP threads (up to physical core count)</li> <li>Check if using MPI build when you don't need it (serial is faster for small jobs)</li> <li>Verify <code>max_leaf_count</code> is reasonable (100-300)</li> </ul> All overdensities are -1 <ul> <li>Check that simulation particles are loaded correctly</li> <li>Verify grid points are within simulation box boundaries</li> <li>Ensure kernel radii are appropriate for your simulation resolution</li> </ul> Output file not created <ul> <li>Check <code>Output/filepath</code> directory exists</li> <li>Verify write permissions</li> <li>Look for error messages about grid points (might be 0 grid points)</li> </ul>"},{"location":"runtime-arguments/","title":"Runtime Arguments","text":"<p>Command line arguments for controlling gridder execution.</p>"},{"location":"runtime-arguments/#synopsis","title":"Synopsis","text":"<pre><code>parent_gridder &lt;parameter_file&gt; &lt;nthreads&gt; [snapshot_number] [verbosity]\n</code></pre>"},{"location":"runtime-arguments/#arguments","title":"Arguments","text":""},{"location":"runtime-arguments/#parameter_file","title":"<code>parameter_file</code>","text":"<p>Required \u2022 Position 1</p> <p>Type: String (file path) Description: Path to YAML parameter configuration file</p> <p>Example: <pre><code>./parent_gridder params.yml 8\n./parent_gridder /path/to/config/my_params.yml 4\n</code></pre></p> <p>Notes:</p> <ul> <li>Must be a valid YAML file</li> <li>See Parameters for file format</li> <li>Relative or absolute paths supported</li> </ul>"},{"location":"runtime-arguments/#nthreads","title":"<code>nthreads</code>","text":"<p>Required \u2022 Position 2</p> <p>Type: Integer Range: 1 to hardware limit Description: Number of OpenMP threads to use</p> <p>Example: <pre><code>./parent_gridder params.yml 8   # Use 8 OpenMP threads\n./parent_gridder params.yml 16  # Use 16 OpenMP threads\n</code></pre></p> <p>Recommendations:</p> Single-Node BuildMPI Build <pre><code># Use number of physical cores (not hyperthreads)\n# Check with: lscpu | grep \"^CPU(s):\"\n./parent_gridder params.yml 8\n</code></pre> <pre><code># Threads per rank \u00d7 ranks = total cores\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./parent_gridder params.yml 1\n# Note: nthreads argument ignored in MPI mode\n# Use OMP_NUM_THREADS environment variable instead\n</code></pre> <p>Performance Notes:</p> <ul> <li>More threads \u2260 always faster</li> <li>Sweet spot: Number of physical cores</li> <li>Hyperthreading (2\u00d7 cores) usually gives &lt;20% speedup</li> <li>Measure performance for your specific case</li> </ul> <p>Auto-Detection:</p> <p>The gridder will auto-detect available cores, but explicit specification is recommended for reproducibility:</p> <pre><code># Let OpenMP choose (may use all cores including hyperthreads)\n./parent_gridder params.yml 1\n\n# Better: Explicitly set to physical core count\n./parent_gridder params.yml $(lscpu | grep \"^Core(s)\" | awk '{print $4}')\n</code></pre>"},{"location":"runtime-arguments/#snapshot_number","title":"<code>snapshot_number</code>","text":"<p>Optional \u2022 Position 3</p> <p>Type: Integer Range: \u2265 0 Default: None (no replacement) Description: Snapshot number for placeholder replacement in file paths</p> <p>Example: <pre><code># Process snapshot 42\n./parent_gridder params.yml 8 42\n</code></pre></p> <p>Behavior:</p> <p>When provided, replaces all occurrences of the placeholder string in:</p> <ul> <li><code>Input/filepath</code></li> <li><code>Output/basename</code></li> <li><code>Grid/grid_file</code> (if <code>type: file</code>)</li> </ul> <p>Parameter File: <pre><code>Input:\n  filepath: /data/snapshot_0000.hdf5\n  placeholder: \"0000\"\n\nOutput:\n  basename: grid_0000.hdf5\n</code></pre></p> <p>Command: <pre><code>./parent_gridder params.yml 8 42\n</code></pre></p> <p>Actual Paths: <pre><code>Input:  /data/snapshot_0042.hdf5\nOutput: grid_0042.hdf5\n</code></pre></p> <p>Zero-Padding:</p> <p>The snapshot number is zero-padded to match placeholder length:</p> Placeholder Snapshot Result <code>\"000\"</code> 5 <code>005</code> <code>\"0000\"</code> 5 <code>0005</code> <code>\"0000\"</code> 42 <code>0042</code> <code>\"0000\"</code> 1234 <code>1234</code> <p>No Placeholder:</p> <p>If no <code>Input/placeholder</code> is specified in parameters, this argument has no effect (paths used as-is).</p>"},{"location":"runtime-arguments/#verbosity","title":"<code>verbosity</code>","text":"<p>Optional \u2022 Position 4</p> <p>Type: Integer Range: 0, 1, or 2 Default: 1 (rank 0 only) Description: Controls output verbosity level</p> <p>Values:</p> Level Description Use Case 0 Errors only Production runs, minimal logging 1 Rank 0 only (default) Normal use, clean MPI output 2 All ranks print Debugging MPI issues <p>Examples:</p> Minimal Output (0)Default (1)All Ranks (2) <pre><code>./parent_gridder params.yml 8 42 0\n</code></pre> <p>Output: Only errors printed <pre><code>[ERROR] Failed to open file: snapshot_0042.hdf5\n</code></pre></p> <pre><code>./parent_gridder params.yml 8 42 1\n# Or omit (default):\n./parent_gridder params.yml 8 42\n</code></pre> <p>Output: Normal progress messages from rank 0 only (in MPI) <pre><code>[INFO] Reading snapshot from /data/snapshot_0042.hdf5\n[INFO] Creating 1000000 grid points\n[INFO] Building octree...\n[TIMER] Octree construction: 2.341s\n</code></pre></p> <pre><code>mpirun -n 4 ./parent_gridder params.yml 1 42 2\n</code></pre> <p>Output: All MPI ranks print (can be very verbose!) <pre><code>[RANK 0] Loading particles for local cells\n[RANK 1] Loading particles for local cells\n[RANK 2] Loading particles for local cells\n[RANK 3] Loading particles for local cells\n...\n</code></pre></p> <p>Serial Mode:</p> <p>In single-node builds, levels 1 and 2 are equivalent (all output printed).</p> <p>MPI Mode:</p> <ul> <li>Level 0: Suppresses non-error output from all ranks</li> <li>Level 1: Only rank 0 prints (keeps output clean)</li> <li>Level 2: All ranks print (useful for debugging MPI-specific issues)</li> </ul> <p>Verbosity 2 in MPI</p> <p>With many ranks, verbosity=2 produces overwhelming output. Use only for debugging with small rank counts (&lt;10).</p>"},{"location":"runtime-arguments/#full-usage-examples","title":"Full Usage Examples","text":""},{"location":"runtime-arguments/#basic-execution","title":"Basic Execution","text":"<pre><code># Minimal required arguments\n./parent_gridder params.yml 8\n\n# With snapshot number\n./parent_gridder params.yml 8 42\n\n# With verbosity control\n./parent_gridder params.yml 8 42 0\n</code></pre>"},{"location":"runtime-arguments/#mpi-execution","title":"MPI Execution","text":"<pre><code># Basic MPI run (4 ranks, 2 threads each = 8 cores)\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1 42\n\n# With custom verbosity\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1 42 1\n\n# Debugging with full output\nexport OMP_NUM_THREADS=1\nmpirun -n 2 ./build_mpi/parent_gridder params.yml 1 42 2\n</code></pre>"},{"location":"runtime-arguments/#production-workflow","title":"Production Workflow","text":"<pre><code>#!/bin/bash\n# Process multiple snapshots\n\nPARAMS=\"production_params.yml\"\nNTHREADS=16\n\nfor SNAP in {0..100}; do\n    echo \"Processing snapshot $SNAP\"\n    ./parent_gridder $PARAMS $NTHREADS $SNAP 0\n    if [ $? -ne 0 ]; then\n        echo \"ERROR: Snapshot $SNAP failed\"\n        exit 1\n    fi\ndone\n</code></pre>"},{"location":"runtime-arguments/#hpc-batch-script","title":"HPC Batch Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=gridder\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-node=16\n#SBATCH --cpus-per-task=2\n#SBATCH --time=02:00:00\n\n# Load modules\nmodule load gcc/11.2\nmodule load hdf5/1.12\nmodule load openmpi/4.1\n\n# Set OpenMP threads (matches cpus-per-task)\nexport OMP_NUM_THREADS=2\n\n# 4 nodes \u00d7 16 ranks/node \u00d7 2 threads/rank = 128 cores total\nmpirun ./parent_gridder params.yml 1 ${SNAP_NUM} 1\n</code></pre>"},{"location":"runtime-arguments/#help-and-version","title":"Help and Version","text":""},{"location":"runtime-arguments/#display-help","title":"Display Help","text":"<pre><code>./parent_gridder --help\n./parent_gridder -h\n</code></pre> <p>Output: <pre><code>FLARES-2 Parent Gridder\nVersion: 0.1.0\nGit: abc1234 (main)\nBuild: Release with OpenMP + MPI\n\nUsage: parent_gridder &lt;parameter_file&gt; &lt;nthreads&gt; [snapshot_number] [verbosity]\n\nArguments:\n  parameter_file   Path to YAML parameter configuration file\n  nthreads        Number of OpenMP threads (1-64)\n  snapshot_number Optional snapshot number (\u22650, replaces placeholder)\n  verbosity       Optional verbosity level: 0=errors only, 1=rank 0 only (default), 2=all ranks\n\nExamples:\n  parent_gridder params.yml 8\n  parent_gridder params.yml 16 42\n  parent_gridder params.yml 8 0 2  # All ranks print\n  mpirun -n 4 parent_gridder params.yml 8\n\nFor more information: https://github.com/flaresimulations/gridder\n</code></pre></p>"},{"location":"runtime-arguments/#check-version","title":"Check Version","text":"<pre><code>./parent_gridder --version\n</code></pre> <p>Output: <pre><code>FLARES-2 Parent Gridder\nVersion: 0.1.0\nGit commit: abc1234def567890\nBranch: main\nBuild date: 2024-01-15\nCompiler: GNU 11.2.0\nMPI: Enabled (OpenMPI 4.1.2)\nOpenMP: Enabled (4.5)\nHDF5: 1.12.1\n</code></pre></p>"},{"location":"runtime-arguments/#exit-codes","title":"Exit Codes","text":"Code Meaning Common Causes 0 Success Normal completion 1 Error Parameter parsing, file I/O, runtime errors 2 Invalid arguments Wrong number of arguments, invalid values <p>Examples:</p> <pre><code>./parent_gridder params.yml 8\necho $?  # 0 if successful, 1 if error\n\n# Check for errors in scripts\n./parent_gridder params.yml 8 42 || echo \"Gridder failed!\"\n</code></pre>"},{"location":"runtime-arguments/#environment-variables","title":"Environment Variables","text":""},{"location":"runtime-arguments/#omp_num_threads","title":"<code>OMP_NUM_THREADS</code>","text":"<p>Description: Number of OpenMP threads (alternative to <code>nthreads</code> argument)</p> <p>Note: In MPI builds, use this instead of the <code>nthreads</code> argument.</p> <pre><code># Single-node: Use argument\n./parent_gridder params.yml 8\n\n# MPI: Use environment variable\nexport OMP_NUM_THREADS=2\nmpirun -n 4 ./parent_gridder params.yml 1\n</code></pre>"},{"location":"runtime-arguments/#omp_proc_bind","title":"<code>OMP_PROC_BIND</code>","text":"<p>Description: Thread affinity policy</p> <p>Recommended: <pre><code>export OMP_PROC_BIND=close\n</code></pre></p> <p>Keeps threads close to parent for better cache performance.</p>"},{"location":"runtime-arguments/#omp_places","title":"<code>OMP_PLACES</code>","text":"<p>Description: Where threads can run</p> <p>Recommended: <pre><code>export OMP_PLACES=cores\n</code></pre></p> <p>Binds threads to physical cores (not hyperthreads).</p>"},{"location":"runtime-arguments/#troubleshooting","title":"Troubleshooting","text":"Invalid number of arguments <p>Error: <pre><code>Invalid number of arguments (1). Expected 2-4 arguments.\n</code></pre></p> <p>Solution: Provide at least parameter file and nthreads: <pre><code>./parent_gridder params.yml 8\n</code></pre></p> Verbosity must be 0, 1, or 2 <p>Error: <pre><code>ERROR: Verbosity must be 0, 1, or 2 (got 3)\n</code></pre></p> <p>Solution: Use valid verbosity level: <pre><code>./parent_gridder params.yml 8 42 1  # Not 3\n</code></pre></p> Snapshot number is not a valid integer <p>Error: <pre><code>ERROR: Snapshot number is not a valid integer\n</code></pre></p> <p>Solution: Ensure snapshot is a number: <pre><code>./parent_gridder params.yml 8 42    # Correct\n./parent_gridder params.yml 8 abc   # Wrong\n</code></pre></p> Which nthreads should I use? <p>Quick answer: Number of physical cores on your system.</p> <p>Find it: <pre><code># Linux\nlscpu | grep \"^Core(s) per socket\" | awk '{print $4}'\n\n# macOS\nsysctl -n hw.physicalcpu\n\n# Use it\nNCORES=$(nproc --all)\n./parent_gridder params.yml $NCORES\n</code></pre></p>"},{"location":"runtime-arguments/#see-also","title":"See Also","text":"<ul> <li>Parameters - Parameter file configuration</li> <li>MPI - Multi-node execution details</li> <li>Quickstart - Getting started guide</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#openmp-thread-control","title":"OpenMP Thread Control","text":"<pre><code>export OMP_NUM_THREADS=8  # Number of OpenMP threads\n</code></pre>"},{"location":"getting-started/configuration/#mpi-configuration","title":"MPI Configuration","text":"<pre><code>export OMP_NUM_THREADS=4\nmpirun -n 2 ./build_mpi/parent_gridder params.yml 1\n# Total cores = 2 ranks \u00d7 4 threads = 8 cores\n</code></pre>"},{"location":"getting-started/configuration/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"getting-started/configuration/#single-node-workstation-16-cores","title":"Single-Node Workstation (16 cores)","text":"<pre><code>export OMP_NUM_THREADS=16\n./build/parent_gridder params.yml 1\n</code></pre>"},{"location":"getting-started/configuration/#hpc-cluster-node-48-cores","title":"HPC Cluster Node (48 cores)","text":"<pre><code># Option 1: Pure OpenMP\nexport OMP_NUM_THREADS=48\n./build/parent_gridder params.yml 1\n\n# Option 2: Hybrid MPI+OpenMP (if grid is very large)\nexport OMP_NUM_THREADS=12\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":""},{"location":"getting-started/installation/#essential-dependencies","title":"Essential Dependencies","text":"<ul> <li>C++20 Compiler: GCC 10+, Clang 12+, or equivalent</li> <li>CMake: Version 3.12 or higher</li> <li>HDF5: Serial HDF5 library (C and HL components)</li> <li>OpenMP: Threading library (usually bundled with compiler)</li> </ul>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li>MPI: For multi-node parallelization (OpenMPI, MPICH, or Intel MPI)</li> </ul>"},{"location":"getting-started/installation/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"getting-started/installation/#macos-homebrew","title":"macOS (Homebrew)","text":"<pre><code># Install dependencies\nbrew install cmake hdf5 libomp\n\n# Optional: Install MPI\nbrew install open-mpi\n</code></pre>"},{"location":"getting-started/installation/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Install dependencies\nsudo apt-get update\nsudo apt-get install build-essential cmake libhdf5-dev\n\n# OpenMP usually comes with GCC\n\n# Optional: Install MPI\nsudo apt-get install libopenmpi-dev openmpi-bin\n</code></pre>"},{"location":"getting-started/installation/#linux-centosrhel","title":"Linux (CentOS/RHEL)","text":"<pre><code># Install dependencies\nsudo yum install gcc-c++ cmake hdf5-devel\n\n# Optional: Install MPI\nsudo yum install openmpi-devel\nmodule load mpi/openmpi-x86_64\n</code></pre>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":""},{"location":"getting-started/installation/#single-node-build-openmp-only","title":"Single-Node Build (OpenMP only)","text":"<pre><code># Clone repository\ngit clone https://github.com/flaresimulations/gridder.git\ncd gridder\n\n# Configure and build\ncmake -B build -DCMAKE_BUILD_TYPE=Release\ncmake --build build -j 8\n\n# Verify build\n./build/parent_gridder --help\n</code></pre>"},{"location":"getting-started/installation/#multi-node-build-mpi-openmp","title":"Multi-Node Build (MPI + OpenMP)","text":"<pre><code># Configure with MPI support\ncmake -B build_mpi -DENABLE_MPI=ON -DCMAKE_BUILD_TYPE=Release\ncmake --build build_mpi -j 8\n\n# Verify build\nmpirun -n 2 ./build_mpi/parent_gridder --help\n</code></pre>"},{"location":"getting-started/installation/#build-types","title":"Build Types","text":"Build Type Flags Description <code>Release</code> (default) <code>-O3 -march=native</code> Maximum performance <code>Debug</code> <code>-g -O0</code> Debugging symbols, assertions enabled <code>RelWithDebInfo</code> <code>-O2 -g</code> Optimized with debug symbols <code>MinSizeRel</code> <code>-Os</code> Optimized for binary size <p>Example with debug build:</p> <pre><code>cmake -B build -DCMAKE_BUILD_TYPE=Debug\ncmake --build build\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":""},{"location":"getting-started/installation/#check-openmp-support","title":"Check OpenMP Support","text":"<pre><code># macOS\notool -L build/parent_gridder | grep omp\n\n# Linux\nldd build/parent_gridder | grep omp\n</code></pre> <p>Expected output: Should show <code>libomp</code> or <code>libgomp</code> linked.</p>"},{"location":"getting-started/installation/#check-mpi-support","title":"Check MPI Support","text":"<pre><code># macOS\notool -L build_mpi/parent_gridder | grep -E 'mpi|omp'\n\n# Linux\nldd build_mpi/parent_gridder | grep -E 'mpi|omp'\n</code></pre> <p>Expected output: Should show both MPI and OpenMP libraries.</p>"},{"location":"getting-started/installation/#run-test-suite","title":"Run Test Suite","text":"<pre><code># Test single-node build\npython tests/test_suite.py --mode serial\n\n# Test multi-node build\npython tests/test_suite.py --mode mpi --ranks 2\n</code></pre> <p>All tests should pass (12/12 for serial, 17/17 for MPI).</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#hdf5-not-found","title":"HDF5 Not Found","text":"<p>If CMake can't find HDF5:</p> <pre><code># Specify HDF5 location explicitly\ncmake -B build -DHDF5_ROOT=/path/to/hdf5\n</code></pre>"},{"location":"getting-started/installation/#openmp-not-found","title":"OpenMP Not Found","text":"<p>If OpenMP detection fails:</p> <pre><code># macOS: Specify libomp location\ncmake -B build -DOpenMP_ROOT=/opt/homebrew/opt/libomp\n\n# Linux: Ensure compiler supports OpenMP\nexport CC=gcc-10\nexport CXX=g++-10\ncmake -B build\n</code></pre>"},{"location":"getting-started/installation/#mpi-compiler-issues","title":"MPI Compiler Issues","text":"<p>If MPI compilers aren't detected:</p> <pre><code># Specify MPI compilers explicitly\ncmake -B build_mpi -DENABLE_MPI=ON \\\n  -DMPI_C_COMPILER=mpicc \\\n  -DMPI_CXX_COMPILER=mpicxx\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide: Run your first simulation</li> <li>Configuration: Set up parameter files</li> </ul>"},{"location":"getting-started/parameters/","title":"Parameter File Reference","text":"<p>The gridder is configured using a YAML parameter file. This page documents all available parameters.</p>"},{"location":"getting-started/parameters/#complete-example","title":"Complete Example","text":"<pre><code>Kernels:\n  nkernels: 5\n  kernel_radius_1: 0.5\n  kernel_radius_2: 1.0\n  kernel_radius_3: 2.0\n  kernel_radius_4: 4.0\n  kernel_radius_5: 8.0\n\nGrid:\n  type: uniform\n  cdim: 100\n\nTree:\n  max_leaf_count: 200\n\nInput:\n  filepath: /path/to/snapshot.hdf5\n  placeholder: \"0000\"\n\nOutput:\n  filepath: ./output/\n  basename: gridded_output.hdf5\n  write_masses: 0\n\nSimulation:\n  gap_fraction: 0.1\n</code></pre>"},{"location":"getting-started/parameters/#parameter-sections","title":"Parameter Sections","text":""},{"location":"getting-started/parameters/#kernels","title":"Kernels","text":"<p>Defines the spherical top hat kernels used for gridding.</p>"},{"location":"getting-started/parameters/#nkernels","title":"<code>nkernels</code>","text":"<ul> <li>Type: Integer</li> <li>Required: Yes</li> <li>Description: Number of kernel radii to use</li> <li>Range: 1-10</li> <li>Example: <code>nkernels: 5</code></li> </ul>"},{"location":"getting-started/parameters/#kernel_radius_n","title":"<code>kernel_radius_N</code>","text":"<ul> <li>Type: Float</li> <li>Required: Yes (for N = 1 to nkernels)</li> <li>Description: Radius of the Nth kernel in simulation units (typically Mpc/h)</li> <li>Units: Comoving distance (same as simulation)</li> <li>Example:   <pre><code>kernel_radius_1: 0.5  # 500 kpc/h\nkernel_radius_2: 1.0  # 1 Mpc/h\nkernel_radius_3: 2.0  # 2 Mpc/h\n</code></pre></li> </ul> <p>Notes: - Kernels are applied simultaneously in a single pass for efficiency - Smaller radii give higher resolution but noisier results - Larger radii give smoother results but lower resolution - Recommended range: 0.1 - 10.0 Mpc/h for cosmological simulations</p>"},{"location":"getting-started/parameters/#grid","title":"Grid","text":"<p>Defines the grid point distribution.</p>"},{"location":"getting-started/parameters/#type","title":"<code>type</code>","text":"<ul> <li>Type: String</li> <li>Required: Yes</li> <li>Options: <code>uniform</code>, <code>random</code>, <code>file</code></li> <li>Description: Grid point distribution type</li> <li>Example: <code>type: uniform</code></li> </ul> <p>Grid Type Details:</p>"},{"location":"getting-started/parameters/#uniform","title":"<code>uniform</code>","text":"<p>Regularly-spaced cubic grid.</p> <p>Required parameters: - <code>cdim</code>: Grid points per dimension (creates cdim\u00b3 total points)</p> <p>Example: <pre><code>Grid:\n  type: uniform\n  cdim: 100  # Creates 100\u00b3 = 1,000,000 grid points\n</code></pre></p>"},{"location":"getting-started/parameters/#random","title":"<code>random</code>","text":"<p>Randomly-distributed points.</p> <p>Required parameters: - <code>n_grid_points</code>: Total number of grid points</p> <p>Example: <pre><code>Grid:\n  type: random\n  n_grid_points: 1000000\n</code></pre></p>"},{"location":"getting-started/parameters/#file","title":"<code>file</code>","text":"<p>Load grid points from a text file.</p> <p>Required parameters: - <code>grid_file</code>: Path to grid point file</p> <p>File format: ASCII text, one point per line <pre><code>x1 y1 z1\nx2 y2 z2\nx3 y3 z3\n</code></pre></p> <p>Example: <pre><code>Grid:\n  type: file\n  grid_file: /path/to/custom_grid.txt\n</code></pre></p>"},{"location":"getting-started/parameters/#cdim","title":"<code>cdim</code>","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for <code>uniform</code> type only)</li> <li>Description: Number of grid points per dimension</li> <li>Total points: cdim\u00b3</li> <li>Typical values: 50-200</li> <li>Example: <code>cdim: 100</code> creates 1,000,000 grid points</li> </ul>"},{"location":"getting-started/parameters/#n_grid_points","title":"<code>n_grid_points</code>","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for <code>random</code> type only)</li> <li>Description: Total number of randomly-distributed grid points</li> <li>Typical values: 100,000 - 10,000,000</li> <li>Example: <code>n_grid_points: 1000000</code></li> </ul>"},{"location":"getting-started/parameters/#grid_file","title":"<code>grid_file</code>","text":"<ul> <li>Type: String (file path)</li> <li>Required: Yes (for <code>file</code> type only)</li> <li>Description: Path to ASCII file containing grid point coordinates</li> <li>Example: <code>grid_file: ./custom_grid.txt</code></li> </ul>"},{"location":"getting-started/parameters/#tree","title":"Tree","text":"<p>Controls the octree spatial partitioning.</p>"},{"location":"getting-started/parameters/#max_leaf_count","title":"<code>max_leaf_count</code>","text":"<ul> <li>Type: Integer</li> <li>Required: Yes</li> <li>Description: Maximum number of particles in a leaf cell before subdivision</li> <li>Range: 50-500</li> <li>Default recommendation: 200</li> <li>Example: <code>max_leaf_count: 200</code></li> </ul> <p>Performance implications: - Lower values (50-100):   - Deeper tree (more memory)   - Faster particle searches   - Better for large particle counts - Higher values (300-500):   - Shallower tree (less memory)   - Slower particle searches   - Better for limited memory</p>"},{"location":"getting-started/parameters/#input","title":"Input","text":"<p>Specifies the input simulation snapshot.</p>"},{"location":"getting-started/parameters/#filepath","title":"<code>filepath</code>","text":"<ul> <li>Type: String (file path)</li> <li>Required: Yes</li> <li>Description: Path to HDF5 snapshot file</li> <li>Format: SWIFT-compatible HDF5</li> <li>Example: <code>filepath: /data/snapshots/snapshot_0042.hdf5</code></li> </ul> <p>Placeholder substitution:</p> <p>If you provide a snapshot number as a command-line argument, the <code>placeholder</code> string is replaced:</p> <pre><code>Input:\n  filepath: /data/snapshots/snapshot_0000.hdf5\n  placeholder: \"0000\"\n</code></pre> <pre><code># Processes snapshot_0042.hdf5\n./build/parent_gridder params.yml 8 42\n</code></pre>"},{"location":"getting-started/parameters/#placeholder","title":"<code>placeholder</code>","text":"<ul> <li>Type: String</li> <li>Required: No</li> <li>Description: String in <code>filepath</code> to replace with snapshot number</li> <li>Default: <code>\"0000\"</code></li> <li>Example: <code>placeholder: \"XXXX\"</code></li> </ul>"},{"location":"getting-started/parameters/#output","title":"Output","text":"<p>Configures output files.</p>"},{"location":"getting-started/parameters/#filepath_1","title":"<code>filepath</code>","text":"<ul> <li>Type: String (directory path)</li> <li>Required: Yes</li> <li>Description: Output directory for gridded data</li> <li>Example: <code>filepath: ./output/</code></li> </ul> <p>Notes: - Directory will be created if it doesn't exist - In MPI mode, each rank writes to this directory</p>"},{"location":"getting-started/parameters/#basename","title":"<code>basename</code>","text":"<ul> <li>Type: String (filename)</li> <li>Required: Yes</li> <li>Description: Base filename for output HDF5 files</li> <li>Example: <code>basename: gridded_output.hdf5</code></li> </ul> <p>Output filenames: - Serial mode: <code>{filepath}/{basename}</code> - MPI mode: <code>{filepath}/{basename_without_ext}_rank_{N}.hdf5</code></p> <p>Examples: <pre><code># Serial: output/gridded_output.hdf5\n# MPI:    output/gridded_output_rank_0.hdf5\n#         output/gridded_output_rank_1.hdf5\n</code></pre></p>"},{"location":"getting-started/parameters/#write_masses","title":"<code>write_masses</code>","text":"<ul> <li>Type: Integer (boolean)</li> <li>Required: No</li> <li>Default: 0</li> <li>Description: Whether to write total mass in each kernel</li> <li>Values:</li> <li><code>0</code>: Write only overdensities</li> <li><code>1</code>: Write both overdensities and total masses</li> <li>Example: <code>write_masses: 1</code></li> </ul> <p>Impact: - Doubles output file size when enabled - Useful for mass-weighted analyses</p>"},{"location":"getting-started/parameters/#simulation","title":"Simulation","text":"<p>Simulation-specific parameters.</p>"},{"location":"getting-started/parameters/#gap_fraction","title":"<code>gap_fraction</code>","text":"<ul> <li>Type: Float</li> <li>Required: No</li> <li>Default: 0.0</li> <li>Description: Fractional gap between top-level cells</li> <li>Range: 0.0 - 0.5</li> <li>Example: <code>gap_fraction: 0.1</code></li> </ul> <p>Purpose: - Adds spacing between top-level cells for debugging - Typically set to 0.0 for production runs - Values &gt; 0 reduce overlap between adjacent cells</p>"},{"location":"getting-started/parameters/#validation","title":"Validation","text":"<p>The parameter parser performs validation:</p> <pre><code>// Missing required parameter\nERROR: Required parameter 'Kernels:nkernels' not found\n\n// Wrong type\nERROR: Parameter 'Grid:cdim' must be an integer\n\n// Invalid kernel count\nERROR: nkernels=15 exceeds maximum of 10\n\n// Missing kernel radius\nERROR: Kernel radius 3 specified but nkernels=2\n</code></pre>"},{"location":"getting-started/parameters/#example-configurations","title":"Example Configurations","text":""},{"location":"getting-started/parameters/#high-resolution-local-analysis","title":"High-Resolution Local Analysis","text":"<pre><code>Kernels:\n  nkernels: 3\n  kernel_radius_1: 0.1  # 100 kpc/h\n  kernel_radius_2: 0.5  # 500 kpc/h\n  kernel_radius_3: 1.0  # 1 Mpc/h\n\nGrid:\n  type: uniform\n  cdim: 200  # 8M grid points\n\nTree:\n  max_leaf_count: 100  # Deep tree for precision\n</code></pre>"},{"location":"getting-started/parameters/#large-volume-survey","title":"Large-Volume Survey","text":"<pre><code>Kernels:\n  nkernels: 5\n  kernel_radius_1: 1.0\n  kernel_radius_2: 2.0\n  kernel_radius_3: 4.0\n  kernel_radius_4: 8.0\n  kernel_radius_5: 16.0\n\nGrid:\n  type: uniform\n  cdim: 100  # 1M grid points\n\nTree:\n  max_leaf_count: 300  # Shallow tree for memory\n</code></pre>"},{"location":"getting-started/parameters/#custom-grid-for-galaxy-positions","title":"Custom Grid for Galaxy Positions","text":"<pre><code>Kernels:\n  nkernels: 2\n  kernel_radius_1: 0.5\n  kernel_radius_2: 2.0\n\nGrid:\n  type: file\n  grid_file: galaxy_positions.txt\n\nTree:\n  max_leaf_count: 200\n</code></pre>"},{"location":"getting-started/parameters/#see-also","title":"See Also","text":"<ul> <li>Quick Start - Getting started tutorial</li> <li>Configuration - Environment configuration guide</li> <li>Installation - Installation instructions</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with the FLARES-2 Gridder in minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the gridder and all dependencies.</p>"},{"location":"getting-started/quickstart/#step-1-create-a-parameter-file","title":"Step 1: Create a Parameter File","text":"<p>Create <code>example_params.yml</code>:</p> <p>```yaml Kernels:   nkernels: 3   kernel_radius_1: 0.5   kernel_radius_2: 1.0   kernel_radius_3: 2.0</p> <p>Grid:   type: uniform   cdim: 50</p> <p>Tree:   max_leaf_count: 200</p> <p>Input:   filepath: /path/to/snapshot.hdf5</p> <p>Output:   filepath: ./output/   basename: gridded_data.hdf5   write_masses: 1</p>"},{"location":"performance/mpi/","title":"MPI Parallelization","text":""},{"location":"performance/mpi/#overview","title":"Overview","text":"<p>The gridder supports MPI for distributed-memory parallelization across multiple nodes. This is combined with OpenMP for a hybrid MPI+OpenMP approach.</p>"},{"location":"performance/mpi/#build-configuration","title":"Build Configuration","text":""},{"location":"performance/mpi/#enabling-mpi","title":"Enabling MPI","text":"<pre><code>cmake -B build_mpi -DENABLE_MPI=ON\ncmake --build build_mpi\n</code></pre>"},{"location":"performance/mpi/#verify-mpi-support","title":"Verify MPI Support","text":"<pre><code># Check for MPI linkage\nldd build_mpi/parent_gridder | grep mpi  # Linux\notool -L build_mpi/parent_gridder | grep mpi  # macOS\n</code></pre>"},{"location":"performance/mpi/#domain-decomposition","title":"Domain Decomposition","text":"<p>The gridder uses spatial domain decomposition to distribute cells across MPI ranks:</p> <ol> <li>Top-level cells are created and distributed across ranks</li> <li>Each rank owns a subset of cells based on spatial partitioning</li> <li>Ghost cells (\"proxy cells\") are exchanged at partition boundaries</li> <li>Each rank processes its domain independently</li> <li>Output is written to separate HDF5 files per rank</li> </ol>"},{"location":"performance/mpi/#partition-strategy","title":"Partition Strategy","text":"<pre><code>// Cells are assigned to ranks based on spatial location\nrank = (cell_x * n_ranks_y * n_ranks_z +\n        cell_y * n_ranks_z +\n        cell_z) % n_ranks\n</code></pre>"},{"location":"performance/mpi/#ghost-cell-exchange","title":"Ghost Cell Exchange","text":"<p>Cells at partition boundaries require particle data from neighboring ranks:</p> <pre><code>// Proxy cells: cells owned by other ranks but needed for kernel overlap\nif (distance_to_boundary &lt; max_kernel_radius) {\n  mark_as_proxy_cell();\n  exchange_particles_with_neighbor_rank();\n}\n</code></pre>"},{"location":"performance/mpi/#exchange-protocol","title":"Exchange Protocol","text":"<ol> <li>Flag phase: Each rank identifies proxy cells needed from neighbors</li> <li>Request phase: Send cell IDs to neighbor ranks</li> <li>Response phase: Neighbor ranks send particle data</li> <li>Integration phase: Received particles used for gridding</li> </ol>"},{"location":"performance/mpi/#running-with-mpi","title":"Running with MPI","text":""},{"location":"performance/mpi/#basic-usage","title":"Basic Usage","text":"<pre><code># 4 MPI ranks, 8 OpenMP threads per rank = 32 total cores\nexport OMP_NUM_THREADS=8\nmpirun -n 4 ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/mpi/#recommended-configurations","title":"Recommended Configurations","text":""},{"location":"performance/mpi/#small-cluster-16-cores-per-node-4-nodes-64-cores","title":"Small Cluster (16 cores per node, 4 nodes = 64 cores)","text":"<pre><code># Option 1: 4 ranks \u00d7 16 threads\nexport OMP_NUM_THREADS=16\nmpirun -n 4 --map-by node ./build_mpi/parent_gridder params.yml 1\n\n# Option 2: 8 ranks \u00d7 8 threads (better load balancing)\nexport OMP_NUM_THREADS=8\nmpirun -n 8 --map-by node:PE=8 ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/mpi/#large-hpc-system-48-cores-per-node-16-nodes-768-cores","title":"Large HPC System (48 cores per node, 16 nodes = 768 cores)","text":"<pre><code># 16 ranks \u00d7 48 threads (one rank per node)\nexport OMP_NUM_THREADS=48\nmpirun -n 16 --map-by node ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/mpi/#process-binding","title":"Process Binding","text":"<p>For NUMA systems, ensure proper binding:</p> <pre><code>export OMP_PROC_BIND=close\nexport OMP_PLACES=cores\nexport OMP_NUM_THREADS=24\nmpirun -n 4 --bind-to socket --map-by socket:PE=24 \\\n  ./build_mpi/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/mpi/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"performance/mpi/#scaling-behavior","title":"Scaling Behavior","text":"Ranks Threads/Rank Total Cores Speedup Efficiency 1 8 8 1.0\u00d7 100% 2 8 16 1.8\u00d7 90% 4 8 32 3.4\u00d7 85% 8 8 64 6.2\u00d7 78% <p>Typical results for 100\u00b3 grid with 10M particles</p>"},{"location":"performance/mpi/#bottlenecks","title":"Bottlenecks","text":"<ol> <li>Ghost cell exchange: Communication overhead at partition boundaries</li> <li>Load imbalance: Non-uniform particle distribution</li> <li>I/O serialization: Each rank reads from shared HDF5 file sequentially</li> </ol>"},{"location":"performance/mpi/#output-files","title":"Output Files","text":"<p>Each MPI rank writes a separate output file:</p> <pre><code>output/gridded_data_rank_0.hdf5\noutput/gridded_data_rank_1.hdf5\noutput/gridded_data_rank_2.hdf5\noutput/gridded_data_rank_3.hdf5\n</code></pre>"},{"location":"performance/mpi/#merging-output-optional","title":"Merging Output (Optional)","text":"<p>While not required, you can merge rank files for convenience:</p> <pre><code>import h5py\nimport numpy as np\n\n# Example: Concatenate grid points from all ranks\nall_coords = []\nall_overdensities = {}\n\nfor rank in range(n_ranks):\n    with h5py.File(f'gridded_data_rank_{rank}.hdf5', 'r') as f:\n        all_coords.append(f['Grids/GridPointCoordinates'][:])\n\n        for kernel in f['Grids'].keys():\n            if kernel.startswith('Kernel_'):\n                if kernel not in all_overdensities:\n                    all_overdensities[kernel] = []\n                all_overdensities[kernel].append(\n                    f[f'Grids/{kernel}/GridPointOverDensities'][:]\n                )\n\ncoords = np.concatenate(all_coords)\nfor kernel, data in all_overdensities.items():\n    all_overdensities[kernel] = np.concatenate(data)\n</code></pre>"},{"location":"performance/mpi/#troubleshooting","title":"Troubleshooting","text":""},{"location":"performance/mpi/#mpi-hangs-at-startup","title":"MPI Hangs at Startup","text":"<p>Check that all ranks can access the HDF5 file:</p> <pre><code># Test file access from all nodes\nmpirun -n 4 ls -l /path/to/snapshot.hdf5\n</code></pre>"},{"location":"performance/mpi/#memory-exhaustion","title":"Memory Exhaustion","text":"<p>Reduce particles per rank: - Increase number of ranks - Reduce <code>max_leaf_count</code> to limit tree depth - Process fewer kernel radii per run</p>"},{"location":"performance/mpi/#load-imbalance","title":"Load Imbalance","text":"<p>Monitor rank workload:</p> <pre><code>mpirun -n 4 ./build_mpi/parent_gridder params.yml 1 2&gt;&amp;1 | grep \"particles processed\"\n</code></pre> <p>If ranks show significantly different particle counts, the domain decomposition may be unbalanced due to clustered particle distribution.</p>"},{"location":"performance/mpi/#see-also","title":"See Also","text":"<ul> <li>OpenMP Threading - Thread-level parallelization</li> <li>Configuration Guide - Environment setup</li> <li>Parameter Reference - YAML configuration options</li> </ul>"},{"location":"performance/openmp/","title":"OpenMP Threading","text":""},{"location":"performance/openmp/#overview","title":"Overview","text":"<p>Both build configurations use OpenMP for intra-node parallelization:</p> <ul> <li>Single-node build: OpenMP only (multi-threaded)</li> <li>Multi-node build: MPI + OpenMP hybrid</li> </ul>"},{"location":"performance/openmp/#current-parallelization","title":"Current Parallelization","text":""},{"location":"performance/openmp/#position-unpacking-srccellcpp593","title":"Position Unpacking (src/cell.cpp:593)","text":"<p>Parallel unpacking of 3D position arrays from flat HDF5 data:</p> <pre><code>#pragma omp parallel for schedule(static)\nfor (size_t p = 0; p &lt; chunk.particle_count; p++) {\n  chunk.positions[p] = {pos_flat[p * 3], pos_flat[p * 3 + 1],\n                        pos_flat[p * 3 + 2]};\n}\n</code></pre> <p>Speedup: ~T\u00d7 where T = number of threads Applies to: Both serial and MPI builds</p>"},{"location":"performance/openmp/#performance-tuning","title":"Performance Tuning","text":""},{"location":"performance/openmp/#thread-count-selection","title":"Thread Count Selection","text":"<pre><code># Auto-detect (recommended for single-node)\n./build/parent_gridder params.yml 1\n\n# Manual control\nexport OMP_NUM_THREADS=8\n./build/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/openmp/#binding-strategy","title":"Binding Strategy","text":"<p>For NUMA systems:</p> <pre><code>export OMP_PROC_BIND=close\nexport OMP_PLACES=cores\nexport OMP_NUM_THREADS=8\n./build/parent_gridder params.yml 1\n</code></pre>"},{"location":"performance/openmp/#expected-performance","title":"Expected Performance","text":"Component Parallelization Speedup (8 threads) HDF5 I/O Sequential 1\u00d7 (bottleneck) Position unpacking OpenMP ~8\u00d7 Cell processing Sequential 1\u00d7 Overall Hybrid ~1.1-1.2\u00d7 <p>The modest overall speedup is due to Amdahl's Law: HDF5 I/O dominates (~60-80% of time) and cannot be parallelized with standard HDF5.</p>"}]}